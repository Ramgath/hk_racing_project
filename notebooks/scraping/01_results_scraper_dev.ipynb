{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49704ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527aa4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "date = \"2025/04/20\"\n",
    "course = \"ST\"\n",
    "max_races = 10\n",
    "\n",
    "race_pages = []\n",
    "\n",
    "for race in range(1, max_races + 1):\n",
    "    attempt = 1\n",
    "    max_attempts = 3\n",
    "    success = False\n",
    "    timeouts = [10, 20, 30]\n",
    "\n",
    "    while attempt <= max_attempts and not success:\n",
    "        url = f\"https://racing.hkjc.com/racing/information/English/Racing/LocalResults.aspx?RaceDate={date}&Racecourse={course}&RaceNo={race}\"\n",
    "        try:\n",
    "            print(f\"üîÑ Attempt {attempt} for Race {race} (timeout: {timeouts[attempt - 1]}s)...\")\n",
    "            response = requests.get(url, headers=headers, timeout=timeouts[attempt - 1])\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"lxml\")\n",
    "                race_pages.append(soup)\n",
    "                print(f\"‚úÖ Race {race} loaded successfully.\")\n",
    "                success = True\n",
    "            else:\n",
    "                print(f\"‚ùå Race {race} returned status {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Race {race} failed on attempt {attempt}: {e}\")\n",
    "        \n",
    "        if not success:\n",
    "            attempt += 1\n",
    "            time.sleep(3)  # Small pause between retries\n",
    "\n",
    "    if not success:\n",
    "        print(f\"‚ùå‚ùå Race {race} failed after {max_attempts} attempts. Exiting loop.\")\n",
    "        break  # Hard fail ‚Äî don‚Äôt continue if any race is missed\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(race_pages)} races before exit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_race_details(soup):\n",
    "    race_tab = soup.find(\"div\", class_=\"race_tab\")\n",
    "    if not race_tab:\n",
    "        print(\"‚ùå Could not find race_tab div\")\n",
    "        return {}\n",
    "\n",
    "    rows = race_tab.find_all(\"tr\")\n",
    "\n",
    "    try:\n",
    "        # --- Row 1 ---\n",
    "        race_number_text = rows[0].find(\"td\").text.strip()\n",
    "        race_number = int(re.search(r\"RACE\\s+(\\d+)\", race_number_text).group(1))\n",
    "\n",
    "        # --- Row 2 ---\n",
    "        class_distance_rating = rows[2].find_all(\"td\")[0].text.strip()\n",
    "        going = rows[2].find_all(\"td\")[2].text.strip()\n",
    "\n",
    "        # Parse using regex\n",
    "        rating_part = \"Unrestricted\"  # default\n",
    "        match = re.match(r\"(Class \\d+)\\s*-\\s*(\\d+)M(?:\\s*-\\s*\\((.+?)\\))?\", class_distance_rating)\n",
    "\n",
    "        if not match:\n",
    "            raise ValueError(f\"Unable to parse class/distance/rating: '{class_distance_rating}'\")\n",
    "\n",
    "        class_part = match.group(1)\n",
    "        distance_part = int(match.group(2))\n",
    "        if match.group(3):  # Only if rating is present\n",
    "            rating_part = match.group(3)\n",
    "\n",
    "        # --- Row 3 ---\n",
    "        race_name = rows[3].find_all(\"td\")[0].text.strip()\n",
    "        course_config = rows[3].find_all(\"td\")[2].text.strip()\n",
    "\n",
    "        # --- Row 4 ---\n",
    "        prize_raw = rows[4].find_all(\"td\")[0].text.strip()\n",
    "        prize_clean = int(re.sub(r\"[^\\d]\", \"\", prize_raw))  # remove HK$ and commas\n",
    "\n",
    "        return {\n",
    "            \"race_number\": race_number,\n",
    "            \"race_class\": class_part,\n",
    "            \"distance_m\": distance_part,\n",
    "            \"rating_bracket\": rating_part,\n",
    "            \"going\": going,\n",
    "            \"race_name\": race_name,\n",
    "            \"course_config\": course_config,\n",
    "            \"prize_hkd\": prize_clean\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to parse race details: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "race_details_list = []\n",
    "\n",
    "for soup in race_pages:\n",
    "    details = parse_race_details(soup)\n",
    "    if details:\n",
    "        race_details_list.append(details)\n",
    "\n",
    "race_details_df = pd.DataFrame(race_details_list)\n",
    "race_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def safe_int(text):\n",
    "    try:\n",
    "        return int(text.strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def safe_float(text):\n",
    "    try:\n",
    "        return float(text.strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_runners_table(soup):\n",
    "    performance_div = soup.find(\"div\", class_=\"performance\")\n",
    "    if not performance_div:\n",
    "        print(\"‚ùå Could not find performance div\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = performance_div.find(\"tbody\").find_all(\"tr\")\n",
    "    data = []\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "\n",
    "        # Skip if no win odds (i.e., didn't start)\n",
    "        win_odds_raw = cells[11].text.strip()\n",
    "        if win_odds_raw == \"\" or win_odds_raw in {\"---\", \"-\"}:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            placing = safe_int(cells[0].text)\n",
    "            horse_no = safe_int(cells[1].text)\n",
    "\n",
    "            horse_info = cells[2].text.strip()\n",
    "            horse_name = horse_info.split(\"(\")[0].strip()\n",
    "            horse_id = horse_info.split(\"(\")[1].replace(\")\", \"\").strip() if \"(\" in horse_info else None\n",
    "\n",
    "            jockey = cells[3].text.strip()\n",
    "            trainer = cells[4].text.strip()\n",
    "            act_weight = safe_int(cells[5].text)\n",
    "            declared_weight = safe_int(cells[6].text)\n",
    "            draw = safe_int(cells[7].text)\n",
    "\n",
    "            # Convert finish time (e.g., 1:39.62) ‚Üí 99.62 seconds\n",
    "            finish_time_raw = cells[10].text.strip()\n",
    "            if \":\" in finish_time_raw:\n",
    "                minutes, seconds = map(float, finish_time_raw.split(\":\"))\n",
    "                finish_time_sec = round(minutes * 60 + seconds, 2)\n",
    "            else:\n",
    "                finish_time_sec = None  # e.g., DNF\n",
    "\n",
    "            win_odds = safe_float(win_odds_raw)\n",
    "\n",
    "            data.append({\n",
    "                \"placing\": placing,\n",
    "                \"horse_no\": horse_no,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"jockey\": jockey,\n",
    "                \"trainer\": trainer,\n",
    "                \"actual_weight\": act_weight,\n",
    "                \"declared_weight\": declared_weight,\n",
    "                \"draw\": draw,\n",
    "                \"finish_time_sec\": finish_time_sec,\n",
    "                \"win_odds\": win_odds\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to parse row: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "all_runners = []\n",
    "\n",
    "for i, soup in enumerate(race_pages):\n",
    "    if soup is None:\n",
    "        print(f\"‚ö†Ô∏è Race {i + 1} soup is None. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = parse_runners_table(soup)\n",
    "        if not df.empty:\n",
    "            df[\"race_number\"] = i + 1\n",
    "            all_runners.append(df)\n",
    "            print(f\"‚úÖ Parsed race {i + 1}: {len(df)} runners\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Race {i + 1} has empty DataFrame\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing race {i + 1}: {e}\")\n",
    "\n",
    "df_results = pd.concat(all_runners, ignore_index=True)\n",
    "df_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ca272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_place_dividends(soup, race_number):\n",
    "    div = soup.find(\"div\", class_=\"dividend_tab\")\n",
    "    if not div:\n",
    "        print(f\"‚ö†Ô∏è Race {race_number}: No dividend_tab found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    place_data = []\n",
    "    collecting = False\n",
    "\n",
    "    for row in div.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if not cols:\n",
    "            continue\n",
    "\n",
    "        # Normalize the text of the first cell\n",
    "        first = cols[0].text.strip().upper()\n",
    "\n",
    "        # 1) Start collecting on the row that says \"PLACE\"\n",
    "        if first == \"PLACE\":\n",
    "            collecting = True\n",
    "            # That row has 3 cells: [PLACE, horse_no, payout]\n",
    "            cells = cols[1:]\n",
    "        # 2) If we're already collecting, check if it's a continuation row (2 cells)\n",
    "        elif collecting and len(cols) == 2:\n",
    "            cells = cols\n",
    "        # 3) If a new pool name appears, stop collecting\n",
    "        elif first and collecting:\n",
    "            collecting = False\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Now parse horse_no(s) and payout from `cells`\n",
    "        try:\n",
    "            raw_nos = cells[0].text.strip()\n",
    "            raw_payout = cells[1].text.strip().replace(\",\", \"\")\n",
    "\n",
    "            # Normalize HK$10 ‚Üí HK$1 unit\n",
    "            payout = float(raw_payout) / 10\n",
    "\n",
    "            # Support ties (e.g. \"1,11\")\n",
    "            horse_nos = [int(h) for h in raw_nos.split(\",\")]\n",
    "\n",
    "            for hn in horse_nos:\n",
    "                place_data.append({\n",
    "                    \"horse_no\": hn,\n",
    "                    \"place_dividend_hkd\": payout,\n",
    "                    \"race_number\": race_number\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Race {race_number}: Error parsing PLACE row: {e}\")\n",
    "\n",
    "    return pd.DataFrame(place_data)\n",
    "\n",
    "all_divs = []\n",
    "for i, soup in enumerate(race_pages, start=1):\n",
    "    df_div = parse_place_dividends(soup, race_number=i)\n",
    "    if not df_div.empty:\n",
    "        all_divs.append(df_div)\n",
    "df_dividends = pd.concat(all_divs, ignore_index=True)\n",
    "df_dividends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b744e",
   "metadata": {},
   "source": [
    "#### scraping stewards reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86606702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_stewards_report(date_str: str, retries=3, delay=5) -> pd.DataFrame:\n",
    "    url = f\"https://racing.hkjc.com/racing/information/English/Reports/RaceReportFull.aspx?Date={date_str}\"\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            break\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            attempt += 1\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt} failed for {date_str}: {e}\")\n",
    "            if attempt == retries:\n",
    "                print(f\"‚ùå Failed to load steward report for {date_str} after {retries} attempts.\")\n",
    "                return pd.DataFrame()\n",
    "            time.sleep(delay)\n",
    "\n",
    "    soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "\n",
    "    # --- Metadata ---\n",
    "    meta_divs   = soup.find(\"div\", class_=\"data_meeting\").find_all(\"div\", recursive=False)\n",
    "    meeting_txt = meta_divs[0].get_text(strip=True)\n",
    "    schedule_txt= meta_divs[1].get_text(strip=True)\n",
    "\n",
    "    date_part   = meeting_txt.split(\":\", 1)[1].strip()\n",
    "    date_only, weekday = date_part.split(\" \")\n",
    "    weekday     = weekday.strip(\"()\")\n",
    "    n_races     = int(schedule_txt.split()[0])\n",
    "    start_time  = schedule_txt.split(\": \")[1]\n",
    "\n",
    "    # Convert start_time to 24-hr format\n",
    "    start_time = datetime.strptime(start_time, \"%H:%M\").strftime(\"%H:%M\")\n",
    "\n",
    "    go_block    = soup.find(\"div\", class_=\"data_go\")\n",
    "    course_link = go_block.find(\"a\", class_=\"p2\").get_text(strip=True)\n",
    "    course      = course_link.split(\"-\")[0].strip()\n",
    "\n",
    "    def _extract_two(label: str):\n",
    "        td_label = go_block.find(\"td\", string=lambda t: t and label in t)\n",
    "        td_val = td_label.find_next_sibling(\"td\")\n",
    "        spans = td_val.find_all(\"span\", style=lambda s: s and \"#900\" in s)\n",
    "        time_strings = [sib.strip(\"() \").replace(\"as of\", \"\").strip() for sib in td_val.stripped_strings if \"as of\" in sib]\n",
    "\n",
    "        v1 = float(spans[0].get_text(strip=True))\n",
    "        t1 = datetime.strptime(time_strings[0], \"%I:%M %p\").strftime(\"%H:%M\")\n",
    "        if len(spans) > 1:\n",
    "            v2 = float(spans[1].get_text(strip=True))\n",
    "            t2 = datetime.strptime(time_strings[1], \"%I:%M %p\").strftime(\"%H:%M\")\n",
    "        else:\n",
    "            v2, t2 = None, None\n",
    "        return v1, t1, v2, t2\n",
    "\n",
    "    pen1, pen_t1, pen2, pen_t2 = _extract_two(\"Penetrometer Reading\")\n",
    "    try:\n",
    "        c1, c_t1, c2, c_t2 = _extract_two(\"Clegg Hammer Reading\")\n",
    "    except AttributeError:\n",
    "        c1, c_t1, c2, c_t2 = None, None, None, None\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"meeting_date\":              pd.to_datetime(date_only, dayfirst=True).date(),\n",
    "        \"weekday\":                   weekday,\n",
    "        \"n_races\":                   n_races,\n",
    "        \"start_time\":                start_time,\n",
    "        \"course\":                    course,\n",
    "        \"penetrometer_reading1\":     pen1,\n",
    "        \"penetrometer_time1\":        pen_t1,\n",
    "        \"penetrometer_reading2\":     pen2,\n",
    "        \"penetrometer_time2\":        pen_t2,\n",
    "        \"clegg_reading1\":            c1,\n",
    "        \"clegg_time1\":               c_t1,\n",
    "        \"clegg_reading2\":            c2,\n",
    "        \"clegg_time2\":               c_t2\n",
    "    }])\n",
    "\n",
    "df_track = parse_stewards_report(\"2025/04/16\")\n",
    "if not df_track.empty:\n",
    "    print(df_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620aac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_incidents(soup, date_str):\n",
    "    meeting_date = pd.to_datetime(date_str, dayfirst=False).date()\n",
    "    all_rows = []\n",
    "\n",
    "    # Step 1: Find all race headers (race titles)\n",
    "    race_headers = soup.find_all(\"p\", class_=\"bg_blue\")\n",
    "\n",
    "    for race_header in race_headers:\n",
    "        spans = race_header.find_all(\"span\")\n",
    "        if not spans:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            race_text = spans[0].text.strip()  # \"Race:1 (592)\"\n",
    "            race_number = int(race_text.split(\":\")[1].split(\"(\")[0].strip())\n",
    "            race_number_season = int(race_text.split(\"(\")[1].strip(\")\"))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to parse race header: {race_text} ‚Äî {e}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Get the table that immediately follows this header\n",
    "        incident_table = race_header.find_next(\"table\", class_=\"rirr\")\n",
    "        if not incident_table:\n",
    "            continue\n",
    "\n",
    "        tbody = incident_table.find(\"tbody\")\n",
    "        rows = tbody.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) < 6:\n",
    "                continue\n",
    "\n",
    "            placing = cols[0].text.strip()\n",
    "            horse_no = cols[1].text.strip()\n",
    "\n",
    "            horse_info = cols[3].text.strip()\n",
    "            if \"(\" in horse_info:\n",
    "                horse_name = horse_info.split(\"(\")[0].strip()\n",
    "                horse_id = horse_info.split(\"(\")[1].strip(\")\")\n",
    "            else:\n",
    "                horse_name = horse_info\n",
    "                horse_id = None\n",
    "\n",
    "            jockey = cols[4].text.strip()\n",
    "            incident = cols[5].text.strip()\n",
    "\n",
    "            all_rows.append({\n",
    "                \"meeting_date\": meeting_date,\n",
    "                \"race_number\": race_number,\n",
    "                \"race_number_season\": race_number_season,\n",
    "                \"placing\": placing,\n",
    "                \"horse_no\": horse_no,\n",
    "                \"horse_name\": horse_name,\n",
    "                \"horse_id\": horse_id,\n",
    "                \"jockey\": jockey,\n",
    "                \"incident\": incident\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "url = \"https://racing.hkjc.com/racing/information/English/Reports/RaceReportFull.aspx?Date=2025/04/16\"\n",
    "resp = requests.get(url, headers=headers, timeout=10)\n",
    "soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "\n",
    "df_incidents = parse_incidents(soup, date_str=\"2025/04/16\")\n",
    "df_incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a29707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_stewards_summary(soup, date_str):\n",
    "    meeting_date = pd.to_datetime(date_str).date()\n",
    "    rows = []\n",
    "\n",
    "    # --- Locate both 'other' divs and pick General vs Summary ---\n",
    "    other_divs = soup.find_all(\"div\", class_=\"other\")\n",
    "    general_div = None\n",
    "    summary_div = None\n",
    "\n",
    "    for d in other_divs:\n",
    "        header_p = d.find(\"p\", class_=\"f_fs16 font_wb\")\n",
    "        if not header_p:\n",
    "            continue\n",
    "        label = header_p.get_text(strip=True).lower()\n",
    "        if label == \"general\":\n",
    "            general_div = d\n",
    "        elif label == \"summary\":\n",
    "            summary_div = d\n",
    "\n",
    "    # --- Parse GENERAL entries (all get section=\"General\") ---\n",
    "    if general_div:\n",
    "        # each direct child <div> is one entry\n",
    "        for entry in general_div.find_all(\"div\", recursive=False):\n",
    "            heading = entry.find(\"p\", class_=\"bg_blue\")\n",
    "            content = entry.find(\"p\", class_=\"f_fs16\")\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            # combine heading + content if heading exists\n",
    "            text = content.get_text(separator=\"\\n\", strip=True)\n",
    "            if heading:\n",
    "                text = heading.get_text(strip=True) + \"\\n\" + text\n",
    "\n",
    "            rows.append({\n",
    "                \"meeting_date\": meeting_date,\n",
    "                \"section\": \"General\",\n",
    "                \"content\": text\n",
    "            })\n",
    "\n",
    "    # --- Parse SUMMARY entries (section name from heading) ---\n",
    "    if summary_div:\n",
    "        for entry in summary_div.find_all(\"div\", recursive=False):\n",
    "            heading = entry.find(\"p\", class_=\"bg_blue\")\n",
    "            content = entry.find(\"p\", class_=\"f_fs16\")\n",
    "            if not heading or not content:\n",
    "                continue\n",
    "\n",
    "            # drop the number prefix (\"1. \", \"2. \", etc.)\n",
    "            sec = heading.get_text(strip=True).split(\". \", 1)[-1]\n",
    "            text = content.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            rows.append({\n",
    "                \"meeting_date\": meeting_date,\n",
    "                \"section\": sec,\n",
    "                \"content\": text\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://racing.hkjc.com/racing/information/English/Reports/RaceReportFull.aspx?Date=2025/04/20\"\n",
    "resp = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=10)\n",
    "soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "\n",
    "df_summary = parse_stewards_summary(soup, \"2025/04/20\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae769a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Set credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.path.expanduser(\"~/gcp/credentials/hk_racing_sa_key.json\")\n",
    "\n",
    "# Load CSV\n",
    "csv_path = \"/Users/yastherramgath/hk-racing-project/notebooks/race_details.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Config\n",
    "PROJECT_ID = \"project-benter-428008-b7\"\n",
    "DATASET_ID = \"hk_racing_dataset\"\n",
    "TABLE_NAME = \"race_details\"\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}\"\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Job config\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    autodetect=True,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    ")\n",
    "\n",
    "# Unique job ID (reuse for safety in retry)\n",
    "job_id = f\"upload_{TABLE_NAME}_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Retry with exponential backoff\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        print(f\"üîÅ Attempt {attempt + 1}: Uploading to {table_id} with job ID {job_id}\")\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df, table_id, job_config=job_config, job_id=job_id\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        print(f\"‚úÖ Success: Uploaded {len(df)} rows to {table_id}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Upload failed: {e}\")\n",
    "        wait = 2 ** attempt\n",
    "        print(f\"‚è≥ Waiting {wait} seconds before retrying...\")\n",
    "        time.sleep(wait)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91613c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
