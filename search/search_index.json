{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"decisions/","title":"Key Project Decisions","text":"<p>This document tracks the major design and process decisions for the HK Racing Project, along with context, alternatives considered, and rationale.</p> Date Decision Aspect Alternatives Considered Rationale / Notes 2025-05-04 Use MkDocs + Material theme for docs site Documentation GitHub Pages from <code>/docs</code>, Jekyll Better theming, built-in nav sidebar 2025-05-04 Deploy docs via GitHub Actions to <code>gh-pages</code> CI/CD Manual Pages from <code>/docs</code> folder Full control over build, future plugins 2025-05-05 Design repo for AI-only assistance; skip human-centric features Repository Setup Public repo setup, contributor templates, open-source guidelines Ensures proprietary strategies remain private, limits liability; sharing steps can be added later 2025-05-06 Configure pre-commit hooks for code and notebooks Tooling Manual formatting, commit-by-commit cleaning Automate code style enforcement and strip notebook outputs 2025-05-06 Add GitHub Actions workflows for CI and docs deploy CI/CD Manual test and docs builds Automatically lint, test, and deploy docs on push 2025-05-06 Establish branch protection rules on main Process Unrestricted direct pushes Ensure all changes are reviewed and pass CI before merge 2025-05-06 Create GitHub issue and PR templates Process Freeform issues and PR descriptions Standardize issue reporting and PR reviews 2025-05-06 Scaffold Makefile and setup scripts for environment Developer DX Manual venv creation and pip installs Provide reproducible one-command environment bootstrapping 2025-05-06 Rolled back branch protection rules for personal repo Process Enforced PR and status checks Simplify workflow for solo development; allow direct pushes 2025-05-08 Updated GitHub Pages setup and streamlined milestone tracking Documentation, Project Management Manual tracking, individual project boards Simplify the process with GitHub Issues and checkboxes for clarity and task management 2025-05-08 Remove GitHub Project board and house issues in Markdown Process Online project board Centralize issue tracking within repository docs 2025-05-09 Consolidate all phases plans into 1 master plan Project Managment Have each phase plan in each phase md file Centralize plan 2025-05-09 Phase .md files to house reports and findings only Project Management Treat each phase as a separate project with a dedicated plan Separates plan from results. 2025-05-XX (future decision)"},{"location":"master-plan/","title":"Hong Kong Racing Project: Master Plan","text":"<p>This document outlines the technical plan for building a handicapping and wagering system for thoroughbred horse racing in Hong Kong, with the primary objective of achieving sustainable financial gain through data-driven strategies. It serves as the central reference for project scope, methodology, and planned execution across all phases.</p>"},{"location":"master-plan/#0-project-definition-and-establishment","title":"0. Project Definition and Establishment","text":"<p>(This foundational phase covers the project's aims, documentation strategy, resource configuration, core development environment, and the overall workflow. It is numbered '0' to distinguish it from the sequential execution phases that follow.)</p>"},{"location":"master-plan/#01-introduction-and-aims","title":"0.1. Introduction and Aims","text":""},{"location":"master-plan/#011-project-objective","title":"0.1.1. Project Objective","text":"<ul> <li>Goal: Develop and implement a machine learning-based handicapping and wagering system aimed at generating a positive return on investment (ROI) from Hong Kong horse racing.</li> <li>Team: Solo project with assistance from AI (e.g., Google Gemini).</li> <li>Broader Context: This project focuses on the technical development of a predictive system for financial gain within the domain of horse race wagering. It's acknowledged that this operates within the context of gambling, and responsible practices should guide any potential future application of the system's outputs.</li> </ul>"},{"location":"master-plan/#012-document-purpose","title":"0.1.2. Document Purpose","text":"<p>This <code>master-plan.md</code> (and the accompanying MkDocs site) serves several key purposes:</p> <ul> <li>Centralized Record: Acts as the primary repository for documenting all project plans, methodologies, data sources, and strategic decisions made throughout the project lifecycle.</li> <li>Shared Context for AI Collaboration: Functions as a persistent memory and context for ongoing work and discussions with AI assistants. Referencing and updating this plan and related phase reports ensures continuity.</li> <li>Single Source of Truth: Establishes a definitive reference point for the project's scope, workflow, tools, and challenges.</li> <li>Tracking Evolution: While this document outlines the plan, individual phase reports (<code>docs/phase-XX-name.md</code>) will track the execution and evolution of the project.</li> </ul>"},{"location":"master-plan/#02-resource-and-environment-configuration","title":"0.2. Resource and Environment Configuration","text":""},{"location":"master-plan/#021-hardware","title":"0.2.1. Hardware","text":"<ul> <li>Local machine: MacBook Air, 8GB RAM, Apple Silicon (M1 chip).</li> <li>Online: Standard Colab CPU Environment (e.g., Intel Xeon, ~13 GB RAM). Google Colab free tier also provides variable access to GPU and TPU accelerators.</li> </ul>"},{"location":"master-plan/#022-software-and-cloud-services-primarily-google-ecosystem","title":"0.2.2. Software and Cloud Services (Primarily Google Ecosystem)","text":"<ul> <li>Google BigQuery: Central data warehouse for storing historical and current race data (raw, cleaned, and processed).<ul> <li>Rationale: Chosen for its scalability, powerful SQL interface, direct integration with Python and Colab, and generous free tier (1TB queries/month, 10GB storage/month), making it cost-effective for this project.</li> </ul> </li> <li>Google Cloud Storage (GCS): For storing model artifacts (e.g., pickled models, TensorFlow SavedModel directories) and potentially intermediary data files if needed.<ul> <li>Rationale: Provides a robust, versionable, and accessible storage solution. The free tier (5GB-months standard storage) is expected to be sufficient for model files, offering a significant improvement over local storage or Google Drive for MLOps practices at minimal to no cost.</li> </ul> </li> <li>Vertex AI Model Registry: For registering, versioning, and managing trained machine learning models.<ul> <li>Rationale: Offers a centralized system to track model lineage and metadata, which is crucial for reproducibility and governance. The core model registration functionality is free and integrates with models stored in GCS. Costs would only be incurred if deploying models to Vertex AI Endpoints, which is not the primary plan.</li> </ul> </li> <li>Python Libraries:<ul> <li>Data Acquisition &amp; Interaction: <code>requests</code>, <code>beautifulsoup4</code> (for scraping), <code>google-cloud-bigquery</code>, <code>google-cloud-storage</code>, <code>pandas-gbq</code>.</li> <li>Data Manipulation &amp; Analysis: <code>pandas</code>, <code>numpy</code>, <code>polars</code> (for exploration).</li> <li>Visualization: <code>plotly</code>, <code>matplotlib</code>, <code>seaborn</code>.</li> <li>Machine Learning: <code>scikit-learn</code>, potentially <code>tensorflow</code>, <code>pytorch</code>, <code>xgboost</code>, <code>lightgbm</code> (specific choices to be made in the modeling phase).</li> <li>Web Application (Future Consideration): <code>streamlit</code>.</li> </ul> </li> <li>Version Control: Git, managed via a GitHub repository.</li> <li>Documentation: MkDocs with the Material theme.</li> <li>Configuration Management: YAML files within a <code>config/</code> directory, with sensitive values gitignored.</li> </ul>"},{"location":"master-plan/#023-development-environment-roles","title":"0.2.3. Development Environment Roles","text":"<p>The development environment will leverage the strengths of both local and cloud-based tools:</p> <ul> <li>Visual Studio Code (VS Code) - Local:<ul> <li>Primary Role: Core development environment for most Python scripts, including data ingestion (scraping), data processing, feature engineering modules, utility functions, and the prediction pipeline. Also used for managing the project repository, Git workflows, local testing, and MkDocs site generation/preview.</li> <li>Rationale: Provides a robust and feature-rich local IDE with strong Git integration, debugging capabilities, and direct access to the project's file structure. This is suitable for developing the persistent, production-oriented code in the <code>src/</code> directory.</li> </ul> </li> <li>Google Colab:<ul> <li>Primary Role: Environment for computationally intensive tasks, iterative Exploratory Data Analysis (EDA), rapid prototyping of machine learning models, and training models that benefit from free-tier GPU/TPU access.</li> <li>Rationale: Offers a convenient, browser-based environment with pre-installed libraries and easy access to cloud resources (BigQuery, GCS) and hardware acceleration, making it ideal for experimentation and tasks exceeding local hardware capabilities.</li> </ul> </li> <li>Rationale for Hybrid Approach: This delineation allows for structured code development and project management in VS Code, while retaining the flexibility and power of Colab for research, experimentation, and heavy computation. Data exchange will primarily occur via BigQuery and GCS, ensuring consistency.</li> </ul>"},{"location":"master-plan/#03-proposed-workflow-outline","title":"0.3. Proposed Workflow Outline","text":"<p>This workflow outlines the end-to-end process from data acquisition to prediction generation, leveraging the defined tools and environments.</p>"},{"location":"master-plan/#031-data-ingestion-scraping","title":"0.3.1. Data Ingestion (Scraping)","text":"<ul> <li>Environment: Python scripts developed and executed in VS Code.</li> <li>Process:<ol> <li>Scrape race data (racecards, results, horse details, etc.) from the Hong Kong Jockey Club (HKJC) website using libraries like <code>requests</code> and <code>BeautifulSoup</code>.</li> <li>Perform initial light cleaning and structuring of scraped data within the Python script.</li> <li>Directly load the structured raw data into designated \"raw\" tables in Google BigQuery using the <code>google-cloud-bigquery</code> Python library (e.g., via <code>pandas-gbq</code> or direct API calls).</li> </ol> </li> <li>Data Correction: If discrepancies are found in BigQuery data post-ingestion, they will be addressed by:<ul> <li>Prioritized Method: Dedicated Python scripts (developed in VS Code, stored in <code>src/data_management/</code>) to execute targeted <code>UPDATE</code> statements in BigQuery based on unique row identifiers (e.g., <code>RACE_ID</code>, <code>RUNNER_ID</code>). This ensures changes are auditable and version-controlled.</li> <li>Secondary/Visual Aid (Consideration): A controlled Google Sheet could be used to log desired corrections, which a script then reads to apply to BigQuery. This is not for direct ingestion but as a UI for correction logging if purely script-based updates become cumbersome for many edits.</li> </ul> </li> <li>Rationale for Direct BQ Ingestion:<ul> <li>Robustness &amp; Scalability: Directly loading to BigQuery from Python scripts is more robust and scalable for automated, periodic scraping tasks compared to using Google Sheets as an intermediary. It reduces potential points of failure, schema mismatch issues, and manual intervention.</li> <li>Efficiency: Eliminates the overhead and potential API limitations of writing to and then reading from Sheets.</li> <li>Control: Keeps data handling logic within Python scripts, which are version controlled and part of the main project codebase.</li> </ul> </li> </ul>"},{"location":"master-plan/#032-data-cleansing-preprocessing-and-eda","title":"0.3.2. Data Cleansing, Preprocessing, and EDA","text":"<ul> <li>Environment: Primarily VS Code (using Jupyter notebooks or Python scripts in <code>src/</code>) for systematic cleansing and preprocessing; Google Colab for extensive EDA requiring more interactivity or computational power.</li> <li>Process:<ol> <li>Query raw data from BigQuery into VS Code (or Colab) for analysis.</li> <li>Perform thorough data cleansing (handling missing values, standardizing categories, type enforcement) and preprocessing (encoding, scaling). This logic will be developed as reusable Python functions/modules in <code>src/data_processing/</code> and <code>src/features/</code>.</li> <li>Conduct Exploratory Data Analysis (EDA) to understand data characteristics, identify patterns, and formulate hypotheses. Findings and visualizations will be documented in the relevant <code>docs/phase-XX-name.md</code> files (e.g., <code>docs/phase-03-eda.md</code>).</li> <li>Store cleansed and preprocessed data back into new, dedicated tables/views in BigQuery for use in model development.</li> <li>Store relevant data quality metrics or EDA summary statistics in BigQuery if beneficial for tracking.</li> </ol> </li> </ul>"},{"location":"master-plan/#033-feature-engineering","title":"0.3.3. Feature Engineering","text":"<ul> <li>Environment: Primarily VS Code for defining and scripting feature engineering logic (in <code>src/features/</code>); Colab for iterative development and testing of complex features.</li> <li>Process:<ol> <li>Develop new features based on domain knowledge, EDA insights, and model requirements.</li> <li>Implement feature creation logic as reusable Python functions.</li> <li>Apply feature engineering to the cleansed data from BigQuery, storing the engineered feature sets in new BigQuery tables/views.</li> </ol> </li> </ul>"},{"location":"master-plan/#034-model-development-and-training","title":"0.3.4. Model Development and Training","text":"<ul> <li>Environment: Google Colab (leveraging GPU/TPU access as needed).</li> <li>Process:<ol> <li>Load preprocessed data and engineered features from BigQuery into Colab.</li> <li>Experiment with various machine learning algorithms (from simple baselines to more complex models).</li> <li>Train models, perform hyperparameter tuning, and use rigorous validation techniques (e.g., time-series aware backtesting).</li> <li>Save trained model artifacts (e.g., pickled files, SavedModel formats) to Google Cloud Storage (GCS).<ul> <li>Rationale for GCS over Google Drive: GCS provides better versioning capabilities, programmatic access, and integration with other MLOps tools (like Vertex AI Model Registry). It's a more professional and robust solution for model persistence, and the free tier should cover storage needs.</li> </ul> </li> <li>Register model versions, along with relevant metadata and performance metrics, in Vertex AI Model Registry, pointing to the model artifacts in GCS.<ul> <li>Rationale for Vertex AI Model Registry: Provides a centralized, free-to-use (for registration) service for managing the lifecycle of models, improving organization and reproducibility.</li> </ul> </li> </ol> </li> </ul>"},{"location":"master-plan/#035-prediction-generation","title":"0.3.5. Prediction Generation","text":"<ul> <li>Environment: Python scripts developed and executed in VS Code.</li> <li>Process:<ol> <li>Scrape the latest racecard data (similar to 0.3.1).</li> <li>Preprocess this new data using the same cleansing and feature engineering logic developed in <code>src/</code> and used during training.</li> <li>Load the desired trained model from Google Cloud Storage (its location potentially retrieved via Vertex AI Model Registry).</li> <li>Generate predictions for the upcoming races.</li> <li>Store predictions (e.g., locally as CSV/JSON, or push to a dedicated BigQuery table).</li> </ol> </li> </ul>"},{"location":"master-plan/#036-results-presentation-future-consideration","title":"0.3.6. Results Presentation (Future Consideration)","text":"<ul> <li>Initial: Predictions might be reviewed via generated files (CSV, text) or simple HTML outputs published to GitHub Pages.</li> <li>Future Enhancement: Consider developing a simple interactive dashboard using Streamlit (run locally or deployed) to display predictions, analyze results, or even provide a UI for specific data management tasks.<ul> <li>Rationale for Streamlit: Offers a Python-native way to quickly build data-centric web applications without requiring web development expertise, suitable for creating internal tools or simple dashboards as the project matures.</li> </ul> </li> </ul>"},{"location":"master-plan/#phase-1-data-collection-and-storage","title":"Phase 1: Data Collection and Storage","text":"<p>(This phase focuses on acquiring all necessary raw data and establishing initial storage.)</p>"},{"location":"master-plan/#11-data-sources","title":"1.1. Data Sources","text":"<ul> <li>Primary Source: Hong Kong Jockey Club (HKJC) official website.</li> <li>Data Types: Racecards (upcoming races), race results (historical), horse details, trackwork records, stewards' reports, etc.</li> <li>Historical Data Consideration: An initial dataset was acquired from a third-party vendor (pre-December 2023) with known minor inconsistencies. Data from December 2023 onwards is to be scraped directly. The project will prioritize direct scraping for ongoing data integrity.</li> </ul>"},{"location":"master-plan/#12-data-acquisition-methodology","title":"1.2. Data Acquisition Methodology","text":"<ul> <li>Primary Method: Web scraping using Python libraries (e.g., Beautiful Soup, potentially Playwright/Selenium if needed for dynamic content) executed in Google Colab or local scripts.</li> <li>API Limitations: HKJC does not offer a public API for comprehensive race data, necessitating web scraping.</li> <li>Scope per Session: Scraping will likely be batched (e.g., per race day) to manage load and avoid detection.</li> <li>Maintenance: Scraping scripts will require ongoing maintenance due to potential HKJC website changes.</li> </ul>"},{"location":"master-plan/#13-preliminary-data-formatting-post-scraping","title":"1.3. Preliminary Data Formatting (Post-Scraping)","text":"<ul> <li>Environment: Google Colab or local Python scripts.</li> <li>Objective: Ensure consistency, accuracy, and control over data format before storage. Includes data type enforcement, string manipulation, and basic structural validation.</li> <li>Rationale: Python scripts provide reproducibility and can handle complex logic more robustly than manual formatting or simple spreadsheet functions.</li> </ul>"},{"location":"master-plan/#14-data-storage-strategy","title":"1.4. Data Storage Strategy","text":"<ul> <li>Raw Data Storage: Initially, scraped data might be stored in flat files (JSON, CSV) in Google Cloud Storage or locally.</li> <li>Structured Data Storage: Google BigQuery will serve as the primary data warehouse for cleansed and structured data.<ul> <li>Schema Definition: A detailed data dictionary (see Appendix A) will define table structures and field types in BigQuery.</li> </ul> </li> <li>Intermediary Storage (Original Plan): Google Sheets was considered as an editable central repository before loading to BigQuery. This may be revised for a more direct GCS/local files to BigQuery pipeline.</li> <li>Partitioning: For BigQuery tables, partitioning (e.g., by race date/year) will be considered if data volume grows significantly, to optimize query performance and costs. Not planned initially given projected data sizes.</li> <li>Update Frequency: New race data will be collected and processed typically twice weekly, aligned with the HKJC racing calendar. The process will be automated as much as possible.</li> </ul>"},{"location":"master-plan/#phase-2-data-cleansing-and-preprocessing","title":"Phase 2: Data Cleansing and Preprocessing","text":"<p>(This phase focuses on transforming raw collected data into a clean, consistent, and usable dataset.)</p>"},{"location":"master-plan/#21-overview-of-collected-data","title":"2.1. Overview of Collected Data","text":"<p>The dataset will encompass: * Race identification and context (Date, Course, Race Number, Class, Distance, Going, Prize Money, etc.). * Horse performance metrics (Finishing Place, Finish Time, Sectional Times, Weight Carried, Draw, etc.). * Betting information (Win/Place Odds). * Jockey and Trainer details. * Pre-race horse information (Horse Weight, Rating, Rest Days, Gear, etc.). * (Refer to Appendix A: Data Dictionary for detailed schema).</p>"},{"location":"master-plan/#22-cleansing-procedure","title":"2.2. Cleansing Procedure","text":"<ul> <li>Environment: Primarily Google Colab or local Python scripts, with results stored in BigQuery.</li> <li>Key Steps:<ul> <li>Data Type Enforcement: Ensure columns match predefined types (integer, float, string, date).</li> <li>String Manipulation: Trim whitespace, standardize capitalization, handle special characters.</li> <li>Categorical Value Standardization: Ensure consistency in fields like <code>COURSE</code>, <code>CLASS</code>, <code>GOING</code>. Map variations to standard values.</li> <li>Handling Specific Non-Numeric/Placeholder Strings: Address values like 'UNRATED', 'GRIFFIN', 'DEBUT' in fields that are otherwise numeric or categorical.</li> <li>Addressing Known Inconsistencies: Programmatic fixes for known issues in historical data.</li> </ul> </li> </ul>"},{"location":"master-plan/#23-data-validation-and-format-consistency","title":"2.3. Data Validation and Format Consistency","text":"<ul> <li>Objective: Rigorously validate data against the schema in Appendix A.</li> <li>Checks: Data type verification, value range checks, categorical value consistency, basic relational integrity checks (e.g., consistent IDs).</li> <li>Discrepancy Handling: Refine cleansing scripts or (as a last resort for historical data) document manual corrections.</li> </ul>"},{"location":"master-plan/#24-management-of-missing-valuesoutliers","title":"2.4. Management of Missing Values/Outliers","text":"<ul> <li>Approach: Strategies will be determined based on EDA findings (Phase 3) and model requirements.</li> <li>Missing Values: Options include deletion (rows/columns), mean/median/mode imputation, regression imputation, or model-based imputation.</li> <li>Outliers: Identification (e.g., IQR, Z-scores) followed by potential capping, transformation, or removal.</li> </ul>"},{"location":"master-plan/#25-data-transformation-preparation-for-modeling","title":"2.5. Data Transformation (Preparation for Modeling)","text":"<ul> <li>Encoding Categorical Variables: Convert string categories (e.g., <code>CLASS</code>, <code>GOING</code>) into numerical representations (One-Hot, Label, Target Encoding).</li> <li>Numerical Scaling: Apply standardization or normalization if required by specific models.</li> <li>Other Transformations: Log transforms, polynomial features, etc., based on EDA and model needs.</li> </ul>"},{"location":"master-plan/#phase-3-exploratory-data-analysis-eda","title":"Phase 3: Exploratory Data Analysis (EDA)","text":"<p>(This phase focuses on understanding the data through querying, statistics, and visualizations to uncover patterns and formulate hypotheses.)</p>"},{"location":"master-plan/#31-data-querying","title":"3.1. Data Querying","text":"<ul> <li>Source: Cleansed data tables in Google BigQuery.</li> <li>Environment: Google Colab using the <code>google-cloud-bigquery</code> Python library.</li> <li>Process: Construct SQL queries in Colab, execute against BigQuery, load results into Pandas/Polars DataFrames for analysis.</li> </ul>"},{"location":"master-plan/#32-descriptive-statistics","title":"3.2. Descriptive Statistics","text":"<ul> <li>Tools: Python libraries (Pandas, NumPy) in Colab.</li> <li>Numerical Variables: Calculate count, mean, median, std dev, min, max, percentiles, skewness, kurtosis.</li> <li>Categorical Variables: Frequency counts, unique values, mode.</li> </ul>"},{"location":"master-plan/#33-visualization","title":"3.3. Visualization","text":"<ul> <li>Primary Tool: Plotly for interactive plots in Colab. Seaborn and Matplotlib as alternatives.</li> <li>Univariate Analysis: Histograms, density plots, box plots, bar charts.</li> <li>Bivariate/Multivariate Analysis: Scatter plots, correlation heatmaps, grouped plots, time series plots (if applicable).</li> </ul>"},{"location":"master-plan/#34-initial-observations-and-hypothesis-formulation","title":"3.4. Initial Observations and Hypothesis Formulation","text":"<ul> <li>Synthesize findings from statistics and visualizations.</li> <li>Identify significant patterns, trends, anomalies, and correlations.</li> <li>Formulate initial hypotheses about factors influencing race outcomes.</li> <li>Identify areas needing further investigation. These insights will guide Feature Engineering (Phase 4).</li> </ul>"},{"location":"master-plan/#phase-4-feature-engineering","title":"Phase 4: Feature Engineering","text":"<p>(This phase focuses on creating new, potentially more predictive variables from the cleaned dataset.)</p>"},{"location":"master-plan/#41-methodology","title":"4.1. Methodology","text":"<ul> <li>Environment: Google Colab using Pandas, NumPy, Polars, and Scikit-learn.</li> <li>Approach: Iterative process based on domain knowledge, EDA insights, and preliminary model testing.</li> <li>Goal: Transform base data into a richer feature set that captures complex racing dynamics.</li> </ul>"},{"location":"master-plan/#42-initial-feature-set","title":"4.2. Initial Feature Set","text":"<ul> <li>The starting point is the cleansed, validated dataset (approx. 40-50 core variables as per Appendix A).</li> </ul>"},{"location":"master-plan/#43-target-feature-set","title":"4.3. Target Feature Set","text":"<ul> <li>Strategically expand the initial set. The focus is on developing hypothesized predictive features and validating their impact, rather than a fixed target number of features.</li> </ul>"},{"location":"master-plan/#44-example-categories-of-engineered-features","title":"4.4. Example Categories of Engineered Features","text":"<ul> <li>Form &amp; Consistency: Rolling win/place percentages, average finishing position (recent races), days since last win.</li> <li>Speed &amp; Pace: Calculated speed figures, sectional speed ratings, comparisons to class/distance/track averages.</li> <li>Jockey/Trainer Statistics: Win/place rates (overall, by course, distance, class, horse combination), recent form.</li> <li>Odds-Based Features: Odds movement (if available), probability derived from odds, value indicators (odds vs. finish).</li> <li>Class &amp; Weight Related: Rating relative to class, weight carried relative to past or standard weights.</li> <li>Interaction &amp; Derived Features: Jockey win rate on this course, horse average speed at this distance.</li> <li>Lagged Variables: Performance metrics from the previous race.</li> </ul>"},{"location":"master-plan/#45-feature-engineering-approach","title":"4.5. Feature Engineering Approach","text":"<ul> <li>Iterative: Develop, test (correlation, feature importance from simple models, backtesting), refine/discard.</li> <li>New ideas may emerge during modeling and evaluation.</li> </ul>"},{"location":"master-plan/#46-roles-of-bigquery-and-colab","title":"4.6. Roles of BigQuery and Colab","text":"<ul> <li>BigQuery: Source for cleaned base data. Potentially store validated engineered features in new tables/views for efficient reuse.</li> <li>Colab: Primary environment for implementing complex feature calculation logic.</li> </ul>"},{"location":"master-plan/#phase-5-model-development-and-training","title":"Phase 5: Model Development and Training","text":"<p>(This phase involves selecting, training, and optimizing machine learning models to predict race outcomes.)</p>"},{"location":"master-plan/#51-model-selection","title":"5.1. Model Selection","text":"<ul> <li>Target Variable(s): To be clearly defined (e.g., predict win probability, predict top N finish, predict expected ROI).</li> <li>Baseline Models: Start with simpler, interpretable models (Logistic Regression, Linear Regression, Decision Trees, Random Forests).</li> <li>Advanced Models: Explore Gradient Boosting Machines (XGBoost, LightGBM, CatBoost), Neural Networks (MLPs, potentially RNNs/LSTMs if sequential data like sectional times are heavily used).</li> <li>Selection Criteria: Predictive performance (via backtesting), interpretability, computational cost, training time.</li> </ul>"},{"location":"master-plan/#52-training-environment","title":"5.2. Training Environment","text":"<ul> <li>Primary: Google Colab, leveraging free-tier GPU/TPU accelerators for computationally intensive models.</li> </ul>"},{"location":"master-plan/#53-validation-approach","title":"5.3. Validation Approach","text":"<ul> <li>Primary Method: Rigorous Backtesting.<ul> <li>Simulate training on historical data up to a point and evaluating on subsequent unseen historical data.</li> <li>Employ time-series aware validation (e.g., walk-forward validation) to prevent data leakage.</li> </ul> </li> </ul>"},{"location":"master-plan/#54-hyperparameter-optimization","title":"5.4. Hyperparameter Optimization","text":"<ul> <li>For promising models, optimize hyperparameters to maximize performance on validation sets.</li> <li>Techniques: Grid Search, Randomized Search, Bayesian Optimization (using libraries like Optuna or Scikit-learn tools).</li> </ul>"},{"location":"master-plan/#phase-6-model-evaluation-and-validation","title":"Phase 6: Model Evaluation and Validation","text":"<p>(This phase focuses on rigorously evaluating model performance, especially in the context of wagering profitability.)</p>"},{"location":"master-plan/#61-performance-metrics","title":"6.1. Performance Metrics","text":"<ul> <li>Primary Focus (Wagering Profitability):<ul> <li>Return on Investment (ROI).</li> <li>Hit Rate (Win Prediction Accuracy, Place Prediction Accuracy).</li> </ul> </li> <li>Standard ML Metrics (Task-Dependent):<ul> <li>Classification: Accuracy, Precision, Recall, F1-score, Log Loss, AUC-ROC, AUC-PR.</li> <li>Regression (if predicting rank/time): MAE, RMSE.</li> </ul> </li> </ul>"},{"location":"master-plan/#62-backtesting-outcomes","title":"6.2. Backtesting Outcomes","text":"<ul> <li>Present detailed quantitative results from backtesting simulations.</li> <li>Compare different models and hyperparameter configurations across historical validation periods.</li> </ul>"},{"location":"master-plan/#63-wagering-strategy-simulation","title":"6.3. Wagering Strategy Simulation","text":"<ul> <li>Simulate model use within defined wagering strategies during backtesting.</li> <li>Examples: Fixed Stakes, Percentage Stakes (e.g., fixed percentage of bankroll), Kelly Criterion (adjusting bet size based on perceived edge and odds).</li> <li>Simulation must account for historical odds and race results.</li> </ul>"},{"location":"master-plan/#64-profitability-analysis","title":"6.4. Profitability Analysis","text":"<ul> <li>In-depth analysis of simulated financial performance (total profit/loss, ROI, drawdown, risk-adjusted returns).</li> <li>Assess feasibility of achieving the project's primary objective (positive ROI).</li> </ul>"},{"location":"master-plan/#65-iterative-refinement","title":"6.5. Iterative Refinement","text":"<ul> <li>Evaluation results drive improvements:<ul> <li>Revisit Feature Engineering (Phase 4).</li> <li>Revisit Model Selection (Phase 5.1).</li> <li>Revisit Hyperparameter Optimization (Phase 5.4).</li> <li>Analyze errors to understand model strengths/weaknesses.</li> </ul> </li> </ul>"},{"location":"master-plan/#phase-7-deployment-and-monitoring-future-phase","title":"Phase 7: Deployment and Monitoring (Future Phase)","text":"<p>(This phase outlines operationalizing the model and monitoring its ongoing performance.)</p>"},{"location":"master-plan/#71-prediction-generation-workflow","title":"7.1. Prediction Generation Workflow","text":"<ul> <li>End-to-end process for generating predictions/insights for upcoming race days:<ul> <li>Acquire new racecard data.</li> <li>Apply preprocessing and feature engineering steps consistently.</li> <li>Load trained model.</li> <li>Generate predictions.</li> <li>Store/present predictions for decision-making.</li> </ul> </li> <li>Define automation level and tools.</li> </ul>"},{"location":"master-plan/#72-performance-monitoring","title":"7.2. Performance Monitoring","text":"<ul> <li>Track real-world effectiveness and profitability:<ul> <li>Collect actual race results.</li> <li>Compare outcomes against predictions.</li> <li>Track ROI based on simulated/actual wagers.</li> <li>Monitor for model drift or changes in data distributions.</li> </ul> </li> <li>Define metrics, frequency, and tools.</li> </ul>"},{"location":"master-plan/#73-retraining-approach","title":"7.3. Retraining Approach","text":"<ul> <li>Strategy for periodic model retraining:<ul> <li>Triggers: Performance degradation, fixed schedule, or significant new data accumulation.</li> <li>Process: Reuse pipelines from Phases 2-6 with updated data.</li> <li>Validation: Retrained models validated via backtesting before deployment.</li> </ul> </li> </ul>"},{"location":"master-plan/#phase-8-project-management-and-continuous-improvement","title":"Phase 8: Project Management and Continuous Improvement","text":"<p>(This section covers ongoing project aspects, learnings, and future considerations.)</p>"},{"location":"master-plan/#81-potential-challenges-and-resolutions-ongoing-log","title":"8.1. Potential Challenges and Resolutions (Ongoing Log)","text":"<p>(This list will be dynamic and updated in phase reports or a dedicated decisions/risks log as they arise.) * Cloud Costs (BigQuery/GCP): Monitor usage, optimize queries, leverage free tiers. * Feature Engineering Complexity: Iterative approach, start simple, leverage domain knowledge and EDA. * Model Validation &amp; Profitability: Rigorous backtesting, realistic wagering simulation. * Data Pipeline Robustness: Error handling, logging, monitoring. * Data Integrity &amp; Correction: Ongoing validation checks, clear process for corrections. * Web Scraping Maintenance: Monitor for HKJC site changes, allocate time for script updates.</p>"},{"location":"master-plan/#82-data-size-projections-and-growth","title":"8.2. Data Size Projections and Growth","text":"<ul> <li>Initial dataset (18 years, ~171k rows, 40-50 cols): ~20-50 MB.</li> <li>With engineered features (~150-200 cols): ~50-100 MB.</li> <li>Long-term growth (next 15-20 years): Potentially doubling to ~200-400 MB.</li> <li>These volumes are manageable within BigQuery free tiers and Colab.</li> </ul>"},{"location":"master-plan/#83-tools-summary","title":"8.3. Tools Summary","text":"<ul> <li>Data Storage &amp; Querying: Google BigQuery.</li> <li>Data Processing &amp; Modeling: Python in Google Colab (Pandas, Scikit-learn, etc.).</li> <li>Web Scraping: Python (Beautiful Soup / Playwright).</li> <li>Version Control: Git / GitHub.</li> <li>Documentation: MkDocs (Material theme).</li> <li>Project Management / Task Tracking: <code>docs/project-status.md</code>.</li> <li>(Original) Intermediary Data/Control: Google Sheets, Google Apps Script (may be revised).</li> </ul>"},{"location":"master-plan/#84-future-considerations","title":"8.4. Future Considerations","text":"<ul> <li>Advanced Deployment: If successful, explore more sophisticated deployment (e.g., dedicated prediction server, API).</li> <li>Alternative Data Sources: If HKJC scraping becomes untenable (unlikely to be better external sources).</li> <li>Deeper Model Exploration: More complex architectures if justified by performance.</li> </ul>"},{"location":"master-plan/#85-version-control-strategy","title":"8.5. Version Control Strategy","text":"<ul> <li>All project code, documentation, and configuration files will be version controlled using Git, hosted on a GitHub repository.</li> <li>Branches will be used for feature development and experimentation (e.g., <code>feature/new-scraper</code>, <code>experiment/new-model-arch</code>).</li> <li>The <code>main</code> branch will represent the stable, working version of the project.</li> <li>Commits should be descriptive and atomic.</li> </ul>"},{"location":"master-plan/#86-references","title":"8.6. References","text":"<p>(Placeholder for links to key documentation, research papers, articles, etc., consulted during the project.)</p> <ul> <li>HKJC Website: <code>[Link to be added]</code></li> <li>Pandas Documentation: <code>https://pandas.pydata.org/pandas-docs/stable/</code></li> <li>Scikit-learn Documentation: <code>https://scikit-learn.org/stable/</code></li> <li>MkDocs Material Theme: <code>https://squidfunk.github.io/mkdocs-material/</code></li> </ul>"},{"location":"master-plan/#appendix-a-data-dictionary","title":"Appendix A: Data Dictionary","text":"<p>(This defines the planned schema for data stored in BigQuery. It may evolve.)</p>"},{"location":"master-plan/#a1-table-race_details","title":"A.1. Table: <code>race_details</code>","text":"<p>(Contains details specific to each race event)</p> Column Name Data Type Description Notes/Example <code>RACE_ID</code> INTEGER Unique identifier for each race. Format: <code>YYYYMMDD</code> + <code>RACE_NUM_SEASON</code> (e.g., <code>20240414581</code>). Seasonal race number is 3 digits (e.g., <code>581</code>). <code>DATE</code> DATE Date the race was held. Format: <code>YYYY-MM-DD</code> <code>COURSE</code> STRING Racecourse where the race took place. Values: <code>Sha Tin</code>, <code>Sha Tin (AWT)</code>, <code>Happy Valley</code> <code>RACE_NUM_SEASON</code> INTEGER The sequential number of the race within the racing season. e.g., <code>581</code> <code>CLASS</code> STRING The class of the race. e.g., <code>Class 1</code>, <code>Group 1</code>, <code>Griffin</code> <code>DISTANCE</code> INTEGER The race distance in meters. e.g., <code>1200</code>, <code>1650</code> <code>RANKING</code> STRING The rating bracket for the race. e.g., <code>85-60</code> <code>GOING</code> STRING Description of surface going. e.g., <code>GOOD</code>, <code>YIELDING (WET SLOW)</code> <code>RACE_DESCRIPTION</code> STRING The official title or name of the race. e.g., <code>THE HONG KONG EXCHANGES CHALLENGE CUP HANDICAP</code> <code>TRACK_CONFIG</code> STRING The track configuration for the race. e.g., <code>TURF - \"C\" Course</code>, <code>ALL WEATHER TRACK</code> <code>PRIZE</code> INTEGER The total prize money in HKD for the race. <code>PEN_READING</code> FLOAT Penetrometer reading (turf) or Clegg hammer reading (all-weather track)."},{"location":"master-plan/#a2-table-race_card-or-runners","title":"A.2. Table: <code>race_card</code> (or <code>runners</code>)","text":"<p>(Contains details for each horse participating in a race, pre-race information)</p> Column Name Data Type Description Notes/Example <code>RUNNER_ID</code> STRING Unique identifier for a horse in a specific race. Format: <code>RACE_ID</code> + <code>*</code> + <code>HORSE_ID</code> (e.g., <code>20240414581*CLEARWIN*H255</code>) <code>RACE_ID</code> INTEGER Foreign key referencing <code>race_details.RACE_ID</code>. <code>HORSE_ID</code> STRING Unique identifier for the horse (stable across races). Format: <code>HORSE_NAME</code> (no spaces/special chars) + <code>*</code> + <code>HORSE_CODE</code> (e.g., <code>CLEARWIN*H255</code>) <code>HORSE_NUM</code> INTEGER The number assigned to the horse for that race (usually printed on saddle cloth). e.g., <code>1</code>, <code>2</code>, ... <code>14</code> <code>WEIGHT</code> INTEGER Total weight carried by the horse (lbs), including jockey and gear. <code>HORSE_WEIGHT</code> INTEGER Declared weight of the horse (lbs), usually taken a day or two before the race. <code>DRAW</code> INTEGER Starting gate (barrier) number. Lower numbers are closer to the inside rail. <code>JOCKEY</code> STRING Jockey's name. <code>TRAINER</code> STRING Trainer's name. <code>RATING</code> STRING Official HKJC rating of the horse at the time of the race. Numeric string, or <code>UNRATED</code> (overseas), <code>GRIFFIN</code> (novices). <code>REST_DAYS</code> STRING Number of days since the horse's last run. Numeric string, or <code>DEBUT</code> (first run in HK or ever). <code>RACE_AGE</code> STRING Age of the horse at the time of the race. Numeric string, or <code>UNKNOWN</code>. <code>GEAR</code> STRING Symbols representing gear used by the horse. e.g., <code>PC/XB/TT</code>. See Gear Key below. <p>Gear Key (Example - to be confirmed from HKJC source):</p> <ul> <li><code>B</code>: Blinkers</li> <li><code>CP</code>: Sheepskin Cheek Pieces</li> <li><code>TT</code>: Tongue Tie</li> <li><code>XB</code>: Crossed Nose Band</li> <li><code>P</code>: Pacifier</li> <li><code>V</code>: Visor</li> <li><code>H</code>: Hood</li> <li><code>E</code>: Ear Plugs</li> <li><code>1</code> (suffix): First time using this gear.</li> </ul>"},{"location":"master-plan/#a3-table-race_results","title":"A.3. Table: <code>race_results</code>","text":"<p>(Contains the official results for each horse in a race)</p> Column Name Data Type Description Notes/Example <code>RUNNER_ID</code> STRING Foreign key referencing <code>race_card.RUNNER_ID</code>. <code>RACE_ID</code> INTEGER Foreign key referencing <code>race_details.RACE_ID</code>. <code>HORSE_ID</code> STRING Foreign key referencing horse identity. <code>FINISH_POS</code> INTEGER Final official finishing position. <code>1</code> for winner, <code>2</code> for second, etc. May include codes for non-finishers. <code>STARTING_ODDS</code> FLOAT Decimal odds of the horse just before race start. e.g., <code>1.8</code>, <code>10.5</code> <code>PLACE_PAYOUTS</code> FLOAT Decimal odds payout for a successful place bet. <code>0</code> or <code>NULL</code> if unplaced or no payout. <code>FINISH_TIME</code> FLOAT Finishing time of the horse in seconds. e.g., <code>71.52</code> <code>SEC_TIME_1</code> FLOAT Time taken to finish section 1 (seconds). <code>SEC_TIME_2</code> FLOAT Time taken to finish section 2 (seconds). <code>SEC_TIME_3</code> FLOAT Time taken to finish section 3 (seconds). <code>SEC_TIME_4</code> FLOAT Time taken to finish section 4 (seconds). (Sectional times depend on race distance and course) <code>SEC_TIME_5</code> FLOAT Time taken to finish section 5 (seconds). <code>SEC_TIME_6</code> FLOAT Time taken to finish section 6 (seconds). <code>DIST_BEATEN</code> FLOAT Distance beaten by the winner, in lengths (approx). <code>0</code> for winner. <code>INCIDENTS</code> STRING Notes on any racing incidents involving the horse during the race. e.g., \"Bumped at start\", \"Checked near 800m\""},{"location":"master-plan/#a4-table-horse_register","title":"A.4. Table: <code>horse_register</code>","text":"<p>(Contains static details for all registered horses)</p> Column Name Data Type Description Notes/Example <code>HORSE_ID</code> STRING Unique identifier for the horse. Format: <code>HORSE_NAME</code> (no spaces/specials) + <code>*</code> + <code>HORSE_CODE</code> <code>HORSE_CODE</code> STRING Unique code assigned by HKJC to differentiate horses (esp. same names). e.g., <code>H255</code>, <code>D466</code> <code>HORSE_NAME</code> STRING Official name of the horse. Raw name, e.g., \"DEAN'S ANGEL\" <code>SEX</code> STRING Horse's sex. <code>Gelding</code>, <code>Mare</code>, <code>Colt</code>, <code>Filly</code>, <code>Horse</code>, <code>Rig</code> <code>COLOR</code> STRING Color of the horse. e.g., <code>Bay</code>, <code>Chestnut</code> <code>COUNTRY_OF_ORIGIN</code> STRING Country where the horse was born. e.g., <code>AUS</code>, <code>NZ</code>, <code>IRE</code> <code>IMPORT_TYPE</code> STRING Category of entry into Hong Kong racing. <code>PP</code> (Privately Purchased), <code>PPG</code> (Privately Purchased Griffin), <code>ISG</code> (International Sale Griffin), <code>VIS</code> (Visitor) <code>SIRE</code> STRING The horse's father (sire). <code>DAM</code> STRING The horse's mother (dam). <code>DAM_SIRE</code> STRING The sire of the horse's dam (maternal grandsire). <code>OWNER</code> STRING Name(s) of the horse's owner(s). <p>Import Type Key (Example): * <code>PP</code>: Privately Purchased Horses (previously raced elsewhere). * <code>PPG</code>: Privately Purchased Griffins (unraced young horses). * <code>ISG</code>: International Sale Griffins (unraced horses from approved sales). * <code>VIS</code>: Visiting Invitational Horses (invited for specific major races).</p>"},{"location":"phase-01-collection/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-02-cleansing/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-03-eda/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-04-features/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-05-model/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-06-evaluation/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-07-deployment/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"phase-08-management/","title":"NO REPORT OR FINDINGS GENERATED","text":""},{"location":"project-status/","title":"Project Status","text":"<p>Last Manually Updated: 2025-05-11</p> <p>Overall Project Health: Green</p> <p>Initial setup and planning are progressing well. The new project structure and documentation plan are being finalized. only polishing touches to be done</p> <p>Key Focus for This Week (ending 2005-05-10):</p> <p>M5: Development Environment and Tooling Configured     * Objective: Set up local (VS Code) and cloud (Google Colab) development environments, including Python, necessary libraries, and IDE configurations.</p>"},{"location":"project-status/#general-project-setup","title":"General Project Setup","text":"<p>(Tasks related to overall project infrastructure, documentation, planning, and core technical setup before specific data work begins)</p> <p>Major Milestones:</p> <p>\u2705 M1: Project Repository and Core Structure Established     * Objective: Ensure the GitHub repository is correctly set up with a logical folder structure for code, docs, data, etc.</p> <pre><code>opened: 08 May 2005\ncompleted: 08 May 2025\n</code></pre> <p>\u2705 M2: Master Plan Document (<code>master-plan.md</code>) Finalized     * Objective: Complete and finalize the comprehensive General Project Setup plan in <code>master-plan.md</code> incorporating all decisions from the initial planning phase.</p> <pre><code>opened: 08 May 2005\ncompleted: 09 May 2025\n</code></pre> <p>\u2705 M3: Documentation Site (MkDocs) Configured and Operational     * Objective: Set up MkDocs with the chosen theme, configure navigation in <code>mkdocs.yml</code>, and ensure the site builds and deploys correctly (e.g., to GitHub Pages).</p> <pre><code>opened: 08 May 2005\ncompleted: 09 May 2025\n</code></pre> <p>\u2705  M4: Standard Project Documentation Shells Created     * Objective: Create placeholder markdown files for all planned phases (e.g., <code>phase-01-collection.md</code>, <code>phase-02-cleansing.md</code>, etc.), a <code>decisions.md</code> log, and any other key supporting documentation. Finalize <code>docs/project-status.md</code> structure (current working version).</p> <pre><code>opened: 08 May 2005\ncompleted: 09 May 2025\n</code></pre> <ul> <li> <p>M5: Development Environment and Tooling Configured</p> <ul> <li>Objective: Set up local (VS Code) and cloud (Google Colab) development environments, including Python, necessary libraries, and IDE configurations.</li> </ul> <p>opened: 08 May 2005</p> </li> <li> <p>M6: Cloud Services (GCP) Initial Setup and Access Confirmed</p> <ul> <li>Objective: Set up the Google Cloud Project, enable necessary APIs (BigQuery, Cloud Storage, IAM), and confirm programmatic access (e.g., service account credentials).</li> </ul> <p>opened: 08 May 2005</p> </li> <li> <p>M7: Configuration Management Strategy Implemented</p> <ul> <li>Objective: Establish the <code>config/</code> directory structure with example configuration files and ensure sensitive configurations are correctly handled (e.g., via <code>.gitignore</code>).</li> </ul> <p>opened: 08 May 2005</p> </li> </ul>"},{"location":"project-status/#task-and-issues","title":"Task and Issues","text":"Status Task / Issue Description Milestone Notes / Resolution Priority Date Due Tags \u2705 Set up local Python virtual environment (e.g., <code>venv</code>). M5 High Python; Setup \u2b1c Install core Python libraries from <code>requirements.txt</code> (e.g., pandas, requests, google-cloud-*) M5 Create initial <code>requirements.txt</code>. High Python; Setup \u2b1c Install development libraries from <code>requirements-dev.txt</code> (e.g., pytest, mkdocs, black/flake8). M5 Create initial <code>requirements-dev.txt</code>. Medium Python; Setup \u2b1c Configure VS Code for Python development (interpreter, linter, formatter). M5 Medium Tooling; Setup \u2b1c Confirm access to Google Colab and ability to connect to GCP services. M5 Medium Tooling; GCP \u2705 Create Google Cloud Platform (GCP) project if not already done. M6 High GCP; Setup \u2b1c Enable BigQuery API and Cloud Storage API in GCP console. M6 Also IAM API if managing service accounts. High GCP; Setup \u2705 Create a GCP Service Account with necessary roles for BigQuery and GCS access. M6 E.g., BigQuery Data Editor, Storage Object Admin. High GCP; Security \u2b1c Download service account JSON key and store securely (e.g., in <code>~/gcp/credentials/</code>). M6 Ensure path is gitignored if ever placed near project. Set <code>GOOGLE_APPLICATION_CREDENTIALS</code> env var. High GCP; Security \u2705 Create <code>config/</code> directory at project root. M7 High Config; Setup \u2705 Create <code>config/config.yaml.example</code> with placeholder structure for data sources, GCP project ID, BQ dataset/table names. M7 High Config; Setup \u2705 Create <code>config/config.yaml</code> (gitignored) by copying example and add placeholder values. M7 High Config; Setup \u2705 Add <code>config/config.yaml</code> to <code>.gitignore</code>. M7 Already covered in M1 if done comprehensively. High Git; Config \u2b1c Write basic Python function in <code>src/common/config_loader.py</code> to load <code>config.yaml</code>. M7 Medium Python; Config"},{"location":"project-status/#phase-2-data-cleansing-preprocessing","title":"Phase 2: Data Cleansing &amp; Preprocessing","text":"<p>(Objective: To transform raw data into a clean, consistent, and usable format - see <code>master-plan.md</code> for details)</p> <p>Major Milestones:</p> <p>M1: Define Phase 2 tasks &amp; milestones by reviewing the General Setup Plan and Phase 1 details in <code>master-plan.md</code>.</p>"},{"location":"project-status/#task-and-issues_1","title":"Task and Issues","text":"Status Task / Issue Description Milestone Notes / Resolution Priority Date Due Tags \u2b1c review phase 1 in master-plan.md M1 High doc review --- <p>---</p>"}]}