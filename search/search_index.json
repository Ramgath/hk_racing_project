{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HK Racing Project","text":"<p>Executive Summary The HK Racing Project is a data-driven initiative to ingest, model, and analyze horse racing data from the Hong Kong Jockey Club. Our goal is to build a robust end-to-end pipeline\u2014covering data collection, schema design, pipeline orchestration, and analytics dashboards\u2014to enable rapid iteration on performance metrics and machine-learning models.</p> <p>Objectives - Ingest race results, horse and jockey data, and betting odds into BigQuery. - Define a clear, version-controlled data model and dictionary. - Build an automated pipeline ( Google Sheets + GCS + BigQuery). - Deliver KPI dashboards and exploratory notebooks for analytics and ML.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Phase 1: Project Definition</li> <li>Phase 2: Data Collection &amp; Storage</li> <li>Phase 3: Data Cleansing &amp; Pre-processing</li> <li>Phase 4: Exploratory Data Analysis</li> <li>Phase 5: Feature Engineering</li> <li>Phase 6: Model Development</li> <li>Phase 7: Model Evaluation</li> <li>Phase 8: Deployment &amp; Monitoring</li> <li>Phase 9: Project Management &amp; Documentation</li> <li>Project Status</li> <li>Decisions</li> <li>Milestones</li> </ol>"},{"location":"decisions/","title":"Key Project Decisions","text":"<p>This document tracks the major design and process decisions for the HK Racing Project, along with context, alternatives considered, and rationale.</p> Date Decision Aspect Alternatives Considered Rationale / Notes 2025-05-04 Use MkDocs + Material theme for docs site Documentation GitHub Pages from <code>/docs</code>, Jekyll Better theming, built-in nav sidebar 2025-05-04 Deploy docs via GitHub Actions to <code>gh-pages</code> CI/CD Manual Pages from <code>/docs</code> folder Full control over build, future plugins 2025-05-05 Design repo for AI-only assistance; skip human-centric features Repository Setup Public repo setup, contributor templates, open-source guidelines Ensures proprietary strategies remain private, limits liability; sharing steps can be added later 2025-05-06 Configure pre-commit hooks for code and notebooks Tooling Manual formatting, commit-by-commit cleaning Automate code style enforcement and strip notebook outputs 2025-05-06 Add GitHub Actions workflows for CI and docs deploy CI/CD Manual test and docs builds Automatically lint, test, and deploy docs on push 2025-05-06 Establish branch protection rules on main Process Unrestricted direct pushes Ensure all changes are reviewed and pass CI before merge 2025-05-06 Create GitHub issue and PR templates Process Freeform issues and PR descriptions Standardize issue reporting and PR reviews 2025-05-06 Scaffold Makefile and setup scripts for environment Developer DX Manual venv creation and pip installs Provide reproducible one-command environment bootstrapping 2025-05-06 Rolled back branch protection rules for personal repo Process Enforced PR and status checks Simplify workflow for solo development; allow direct pushes 2025-05-08 Updated GitHub Pages setup and streamlined milestone tracking Documentation, Project Management Manual tracking, individual project boards Simplify the process with GitHub Issues and checkboxes for clarity and task management 2025-05-08 Remove GitHub Project board and house issues in Markdown Process Online project board Centralize issue tracking within repository docs 2025-05-XX (future decision)"},{"location":"milestones/","title":"Milestones &amp; Tasks","text":"<p>This document tracks the key milestones and tasks for the HK Racing project. Each task is broken down with goals, reasons for the task, tags, and expected output. As tasks are completed, links to corresponding GitHub Issues will be added.</p>"},{"location":"milestones/#phase-1-project-definition","title":"Phase 1 \u2013 Project Definition","text":"<ul> <li>[ ] Define Objectives &amp; Success Criteria</li> <li>Goal: Outline the project's primary objectives and success criteria.</li> <li>Why: To clearly define the project\u2019s direction and measurable outcomes.</li> <li>Tags: project, objectives, success criteria</li> <li> <p>Output: A documented list of objectives and success criteria.</p> </li> <li> <p>[ ] Configure Local &amp; Cloud Environments</p> </li> <li>Goal: Set up local development environment, GCP project, and associated services.</li> <li>Why: Ensures all dependencies and cloud services are ready for the project.</li> <li>Tags: setup, environment, GCP, local</li> <li>Output: Fully configured local dev environment and GCP project with necessary access.</li> </ul>"},{"location":"milestones/#phase-2-data-collection-storage","title":"Phase 2 \u2013 Data Collection &amp; Storage","text":"<ul> <li>[ ] Implement Basic Race Results Scraper</li> <li>Goal: Develop a scraper to retrieve race results from HKJC\u2019s website.</li> <li>Why: This scraper will serve as the foundation for data collection.</li> <li>Tags: scraping, race results, HTML</li> <li> <p>Output: A working scraper saving data in raw format (CSV/JSON).</p> </li> <li> <p>[ ] Create Raw Data Storage Schema</p> </li> <li>Goal: Design the folder structure for storing raw scraped data.</li> <li>Why: Consistent organization of data will simplify processing.</li> <li>Tags: storage, data management, schema</li> <li> <p>Output: Defined directory structure for raw data in BigQuery/GCS.</p> </li> <li> <p>[ ] Implement Retry Logic for Scraper Functions</p> </li> <li>Goal: Add retry mechanisms (e.g., tenacity, exponential backoff) to scraper functions.</li> <li>Why: To make the scraper more resilient to network failures and HTML structure changes.</li> <li>Tags: scraping, retry logic, resilience</li> <li>Output: A retry decorator or class that can be reused across scrapers.</li> </ul>"},{"location":"milestones/#phase-3-data-cleansing-pre-processing","title":"Phase 3 \u2013 Data Cleansing &amp; Pre-processing","text":"<ul> <li>[ ] Implement Data Validation Checks</li> <li>Goal: Ensure the scraped data is valid and clean before processing.</li> <li>Why: To ensure that we work with high-quality data from the start.</li> <li>Tags: data quality, validation, cleansing</li> <li>Output: Validation scripts for data checking.</li> </ul>"},{"location":"milestones/#future-phases-to-be-defined-later","title":"Future Phases (To be defined later)","text":"<ul> <li>[ ] Exploratory Data Analysis (EDA)</li> <li>Goal: Perform initial analysis to understand trends and patterns.</li> <li>Why: EDA will guide the feature engineering and modeling steps.</li> <li>Tags: analysis, EDA, data exploration</li> <li>Output: EDA notebook with key insights.</li> </ul>"},{"location":"phase-01-definition/","title":"Phase 1: Project Definition and Establishment","text":""},{"location":"phase-01-definition/#11-introduction-and-aims","title":"1.1 Introduction and Aims","text":""},{"location":"phase-01-definition/#111-project-objective","title":"1.1.1 Project Objective","text":"<ul> <li>Goal: Develop and implement a machine learning-based handicapping and wagering system aimed at generating a positive return on investment (ROI) from Hong Kong horse racing.</li> <li>Team: Solo project with assistance from Google Gemini.</li> <li>Broader Context: This project focuses on the technical development of a predictive system for financial gain within the domain of horse race wagering. It's acknowledged that this operates within the context of gambling, and responsible practices should guide any potential future application of the system's outputs.</li> </ul>"},{"location":"phase-01-definition/#112-document-purpose","title":"1.1.2 Document Purpose","text":"<p>This document serves several key purposes for the Hong Kong Racing Project: * Centralized Record: Acts as the primary repository for documenting all project plans, methodologies, data sources, findings, and decisions made throughout the project lifecycle. This ensures a comprehensive record of the work undertaken. * Shared Memory for AI Collaboration: Functions as a persistent memory and context for ongoing work and discussions with Google Gemini. By referencing and updating this document, particularly a \"Project Status\" section (in <code>docs/project-status.md</code>), we can ensure continuity across multiple sessions, avoid repeating previous discussions, and efficiently build upon prior work, analyses, and conclusions. * Single Source of Truth: Establishes a definitive reference point for the project's scope, workflow, tools, challenges, and results, reducing ambiguity. * Tracking Evolution: Tracks the project's evolution, including changes in strategy, feature engineering approaches, model selection, and key decisions over time. * Workflow for Collaboration:     * Start of Session: The latest version of project documentation will be reviewed.     * Context Review: The \"Project Status\" section will be reviewed to understand the current project state and immediate next steps.     * Execution: Work on the project tasks will proceed based on the reviewed status.     * End of Session Update: The \"Project Status\" section will be updated to reflect tasks completed, decisions made, and the next steps agreed upon before concluding the session. This ensures the documentation always represents the latest project state.</p>"},{"location":"phase-01-definition/#12-resource-and-environment-configuration","title":"1.2 Resource and Environment Configuration","text":""},{"location":"phase-01-definition/#121-hardware","title":"1.2.1 Hardware","text":"<ul> <li>Local machine: MacBook Air, 8GB RAM, Apple Silicon (M1 chip).</li> <li>Online: Standard Colab CPU Environment (e.g., Intel Xeon, ~13 GB RAM).</li> <li>Note: Google Colab free tier also provides access to GPU (often NVIDIA K80) and TPU accelerators, but availability, specific models, and usage limits are variable and not guaranteed.</li> </ul>"},{"location":"phase-01-definition/#122-software-and-cloud-services-google-ecosystem","title":"1.2.2 Software and Cloud Services (Google Ecosystem)","text":"<p>This project primarily utilizes the Google Cloud ecosystem and related tools for data storage, processing, analysis, and machine learning: * Google BigQuery: Serves as the central data warehouse for storing historical and current race data. Used for scalable storage and potentially light data cleaning or querying. Chosen for its scalability, seamless integration with Colab, and cost-effectiveness within the free tier. * Google Colab: The primary environment for data cleaning, analysis, feature engineering, visualization, and machine learning model development and training[cite: 758]. Leverages free tier CPU, GPU, and TPU resources[cite: 759]. Integrates directly with BigQuery and Google Sheets[cite: 759]. * Google Sheets: Used as a central hub for managing data before uploading to BigQuery, potentially for manual input or initial formatting, and for controlling updates via Apps Script[cite: 760]. * Google Apps Script: Planned for automating the biweekly data update process from Google Sheets to BigQuery[cite: 761]. * Google Cloud Storage (GCS): Considered as an alternative or intermediary step for exporting data from BigQuery (as Parquet files) to Colab, potentially avoiding query costs[cite: 762]. * Related Python Libraries:     * Beautiful Soup: For web scraping HKJC data[cite: 763].     * pandas, NumPy, Polars: For data manipulation and feature engineering[cite: 764].     * <code>google-cloud-bigquery</code>: For interacting with BigQuery from Colab[cite: 765].     * Plotly: For interactive data visualization[cite: 765].     * Machine learning libraries (e.g., Scikit-learn, TensorFlow, PyTorch - specific choices mentioned in Phase 6)[cite: 766].</p>"},{"location":"phase-01-definition/#123-development-environment-colab-vs-code","title":"1.2.3 Development Environment (Colab, VS Code)","text":"<p>The project utilizes a combination of cloud-based and local environments for different tasks[cite: 767]: * Google Colab: This is the primary environment for computationally intensive tasks, including[cite: 767]:     * Data cleaning, exploration (EDA), and visualization (using libraries like Plotly)[cite: 767].     * Feature engineering using Python libraries (pandas, NumPy, Polars)[cite: 768].     * Machine learning model development, training, and validation, leveraging Colab's free access to GPU/TPU resources[cite: 768].     * Direct interaction with Google BigQuery for data querying[cite: 769]. * Visual Studio Code (VS Code) - Local: Used on the local machine (MacBook Air) primarily for[cite: 769]:     * Development of helper scripts, such as those interacting with Google Sheets APIs or potentially Google Apps Script development, if needed outside the cloud editor[cite: 769].     * Managing code versions locally before potentially pushing to a repository (though version control is currently implicit via Google Docs in the original plan, now shifting to Git with this repo structure)[cite: 770].</p>"},{"location":"phase-01-definition/#13-proposed-workflow-outline","title":"1.3 Proposed Workflow Outline","text":""},{"location":"phase-01-definition/#131-rationale-for-hybrid-workflow-sheets-bigquery-colab","title":"1.3.1 Rationale for Hybrid Workflow (Sheets -&gt; BigQuery -&gt; Colab)","text":"<p>The chosen workflow utilizes a hybrid approach leveraging Google Sheets, BigQuery (both External and Native tables), and Google Colab[cite: 771]. This specific architecture was selected primarily to balance the critical need for easy data correction with the performance requirements of data analysis and machine learning[cite: 772]. * Ease of Data Correction: The primary driver is the need for a straightforward method to correct data inaccuracies[cite: 773]. Historical data contains known minor inconsistencies[cite: 774]. Using Google Sheets as the primary, editable data source allows for quick manual fixes[cite: 775]. BigQuery External Tables link directly to these sheets[cite: 776]. * Analytical Performance: To overcome performance limitations of querying Sheets directly, data is periodically materialized into a native BigQuery table[cite: 776]. All downstream analytical tasks query this optimized, native table[cite: 777]. * Scalability: Native BigQuery storage provides excellent scalability[cite: 778]. * Resource Efficiency: Computationally intensive tasks are offloaded to Google Colab[cite: 779]. * Integration &amp; Control: Google ecosystem tools integrate effectively[cite: 780]. Google Sheets, potentially with Apps Script, provides centralized control for refreshing the native BigQuery table[cite: 781]. * Cost-Effectiveness: Leverages free tiers of Google Cloud services where possible[cite: 782]. This hybrid model prioritizes straightforward data maintenance via Google Sheets while ensuring query performance for analysis by using native BigQuery tables[cite: 783].</p>"},{"location":"phase-01-definition/#132-alternative-methodologies-considered","title":"1.3.2 Alternative Methodologies Considered","text":"<p>Several alternatives were evaluated and rejected because they didn't adequately balance ease of data correction, analytical performance, scalability, and resource efficiency[cite: 784, 785]: * DuckDB (Local) + Colab:     * Description: Store data locally in DuckDB, process/clean locally, export to Colab for ML[cite: 785].     * Pros: Fast local analytics, free[cite: 786].     * Cons &amp; Why Rejected: Limited by local 8GB RAM; manual data export to Colab; correcting data in DuckDB less straightforward than Sheets[cite: 786, 787]. * Pandas/Polars (Local only):     * Description: Process data entirely in memory locally[cite: 788].     * Pros: Flexible for initial exploration[cite: 788].     * Cons &amp; Why Rejected: Unsustainable on 8GB RAM as data/features grow; lacks BigQuery's persistence and scalability; data correction is ephemeral[cite: 789, 790]. * SQLite (Local):     * Description: Use SQLite as a local database[cite: 791].     * Pros: Lightweight, embedded[cite: 791].     * Cons &amp; Why Rejected: Row-based storage less performant for analytical queries compared to columnar BigQuery; local resource limits still apply[cite: 792, 793]. * PostgreSQL (Local):     * Description: Run a local PostgreSQL server[cite: 793].     * Pros: Robust relational database features[cite: 793].     * Cons &amp; Why Rejected: High overhead and resource demands unsuitable for local hardware[cite: 794, 795]. * Snowflake (Cloud):     * Description: Use Snowflake as a cloud data warehouse[cite: 796].     * Pros: Scalable and performant like BigQuery[cite: 796].     * Cons &amp; Why Rejected: Lacks a comparable free tier to BigQuery, less cost-effective for this project[cite: 797].</p> <p>The hybrid workflow (Sheets -&gt; External BQ Table -&gt; Native BQ Table -&gt; Colab) was selected as it best balances data correction, performance, scalability, resource efficiency, and cost-effectiveness[cite: 798].</p>"},{"location":"phase-02-collection/","title":"Phase 2 \u2013 Data Collection &amp; Storage","text":""},{"location":"phase-02-collection/#21-source-inventory","title":"2.1 Source Inventory","text":"<ul> <li>Racecards pages (upcoming races)</li> <li>Historical result pages (per race)</li> <li>Stewards\u2019 reports pages (meeting summaries)</li> <li>Horse profile pages (breeding, trackwork, vet records, etc.)</li> <li>Betting odds pages (pre-race odds snapshots)</li> </ul>"},{"location":"phase-02-collection/#22-access-authentication","title":"2.2 Access &amp; Authentication","text":"<ul> <li>No API keys required (public website scraping)</li> <li>Respect robots.txt and site terms of service</li> <li>Rate-limit throttling to avoid hammering the site</li> <li>Optional VPN or IP-rotation strategy to mitigate blocking</li> </ul>"},{"location":"phase-02-collection/#23-web-scraping-design","title":"2.3 Web-Scraping Design","text":"<ul> <li>Stack: BeautifulSoup / Selenium / Playwright choice</li> <li>Pagination &amp; dynamic content (JS-rendered pages)</li> <li>Retry logic &amp; exponential back-off on failures</li> <li>HTML parsing: robust CSS/XPath selectors and schema validation</li> </ul>"},{"location":"phase-02-collection/#24-raw-storage-schema","title":"2.4 Raw Storage Schema","text":"<ul> <li>Storage: Google Cloud Storage (GCS) or BigQuery staging</li> <li>Folder layout:   <code>raw/   \u251c\u2500\u2500 racecards/YYYY-MM-DD/   \u251c\u2500\u2500 results/YYYY-MM-DD/   \u251c\u2500\u2500 stewards/YYYY-MM-DD/   \u2514\u2500\u2500 horses/YYYY-MM-DD/</code> -- Formats: JSON or Parquet for downstream processing</li> </ul>"},{"location":"phase-02-collection/#25-incremental-vs-full-loads","title":"2.5 Incremental vs Full Loads","text":"<ul> <li>Full refresh when meeting date changes or schema updates</li> <li>Delta detection via on-page timestamps or URL patterns</li> <li>Schedule: twice weekly (per HKJC calendar) or on-demand before each race meeting</li> </ul>"},{"location":"phase-02-collection/#26-data-ingestion-orchestration","title":"2.6 Data Ingestion Orchestration","text":"<ul> <li>Orchestration: Airflow / Cloud Composer</li> <li>DAG design:</li> <li>Fetch racecards</li> <li>Scrape results &amp; reports</li> <li>Scrape horse profiles &amp; odds</li> <li>Store raw files in GCS / BQ staging</li> <li>Monitoring &amp; Alerts: track success/failure counts, notify on missed scrapes</li> </ul>"},{"location":"phase-02-collection/#27-collection-success-metrics","title":"2.7 Collection Success Metrics","text":"<ul> <li>[ ] \u2265 95% of pages scraped successfully (per meeting)</li> <li>[ ] Retry count \u2264 3 per page before alerting</li> <li>[ ] SLA: all data available within 1 hour of meeting close</li> </ul> <p>Next Steps: 1. Review these headings and adjust any items. 2. Create GitHub Issues for the first tasks (e.g. \u201cImplement racecards scraper\u201d, \u201cSet up raw storage bucket\u201d). 3. As each Issue is closed, fill in its subsection with code snippets and example outputs.</p>"},{"location":"phase-03-cleansing/","title":"Phase 3","text":"<p>Phase\u00a03\u00a0\u2013\u00a0Data\u202fCleansing &amp; Pre\u2011processing (phase-03-cleansing.md)</p> <p>3.1\u202fData\u00a0Profiling &amp; Quality Checks 3.2\u202fStandardization Rules 3.3\u202fHandling Missing &amp; Anomalous Values 3.4\u202fNormalization &amp; Encoding 3.5\u202fPartitioning Strategy (BigQuery) 3.6\u202fAutomated Validation Tests 3.7\u202fCleansed Layer KPIs</p>"},{"location":"phase-04-eda/","title":"Phase 4","text":"<p>Phase\u00a04\u00a0\u2013\u00a0Exploratory\u202fData\u202fAnalysis (phase-04-eda.md)</p> <p>4.1\u202fExploration Notebook Index 4.2\u202fDescriptive Statistics 4.3\u202fRace &amp; Horse Performance Trends 4.4\u202fCorrelation Heatmaps 4.5\u202fFeature Importance Probes 4.6\u202fEDA Findings Summary</p>"},{"location":"phase-05-features/","title":"Phase 5","text":"<p>Phase\u00a05\u00a0\u2013\u00a0Feature Engineering (phase-05-features.md)</p> <p>5.1\u202fFeature List &amp; Definitions 5.2\u202fRolling Window Calculations 5.3\u202fCategorical Encodings 5.4\u202fTarget Variable Construction 5.5\u202fFeature Store Schema 5.6\u202fFeature Quality Metrics</p>"},{"location":"phase-06-model/","title":"Phase 6","text":"<p>Phase\u00a06\u00a0\u2013\u00a0Model Development (phase-06-model.md)</p> <p>6.1\u202fModelling Objectives 6.2\u202fTrain/Validation Split Strategy 6.3\u202fBaseline Models 6.4\u202fHyper\u2011parameter Tuning Plan 6.5\u202fModel Versioning &amp; Registry 6.6\u202fTraining Pipeline Automation</p>"},{"location":"phase-07-evaluation/","title":"Phase 7","text":"<p>Phase\u00a07\u00a0\u2013\u00a0Model Evaluation (phase-07-evaluation.md)</p> <p>7.1\u202fEvaluation Metrics (AUC,\u00a0LogLoss, ROI) 7.2\u202fCross\u2011Validation Results 7.3\u202fBack\u2011testing on Historical Meets 7.4\u202fError Analysis &amp; Bias Checks 7.5\u202fModel Comparison Dashboard 7.6\u202fGo\u2011/No\u2011Go Decision Criteria</p>"},{"location":"phase-08-deployment/","title":"Phase 8","text":"<p>Phase\u00a08\u00a0\u2013\u00a0Deployment &amp; Monitoring (phase-08-deployment.md)</p> <p>8.1\u202fServing Architecture (Batch vs\u00a0Real\u2011time) 8.2\u202fInfrastructure as Code (Terraform) 8.3\u202fCI/CD Release Flow 8.4\u202fOnline Feature Retrieval 8.5\u202fModel Drift Detection 8.6\u202fAlerting &amp; Incident Response Runbook</p>"},{"location":"phase-09-management/","title":"Phase 9","text":"<p>Phase\u00a09\u00a0\u2013\u00a0Project Management &amp; Documentation (phase-09-management.md)</p> <p>9.1\u202fMilestone Tracking &amp; Burndown 9.2\u202fDecisions Log Maintenance 9.3\u202fRisk Register Updates 9.4\u202fBudget &amp; Cost Monitoring 9.5\u202fDocumentation Publishing Workflow 9.6\u202fLessons Learned / Post\u2011mortems 9.7\u202fFuture Roadmap</p>"},{"location":"project-status/","title":"Project Status","text":"<p>Last Manually Updated: 2025-05-08</p> <p>Overall Project Health: Green</p> <p>Initial setup and planning are progressing well. The new project structure and documentation plan are being finalized.</p> <p>Key Focus for This Week (ending 2005-05-09): | Status | Task / Issue Description | Milestone | |--------|---------------------------|-----------| | \ud83d\udd04     | Final review and sign-off on <code>project_plan.md</code>. | M2 |</p>"},{"location":"project-status/#general-project-setup","title":"General Project Setup","text":"<p>(Tasks related to overall project infrastructure, documentation, and planning)</p> <p>Major Milestones: - \u2705 M1: Initial project repository configured (Completed: YYYY-MM-DD) - \u2b1c M2: <code>project_plan.md</code> created and populated from Google Doc (Completed: YYYY-MM-DD) - \u2b1c M3: <code>mkdocs.yml</code> updated for new site structure (Target: YYYY-MM-DD) - \u2b1c M4: All phase report Markdown files named and structured (Target: YYYY-MM-DD) - \u2b1c M5: This <code>project-status.md</code> file structure finalized (Target: YYYY-MM-DD)</p> <p>Detailed Tasks &amp; Issues:</p> Status Task / Issue Description Milestone Notes / Resolution \u2705 Draft initial <code>project_plan.md</code> content. M2 Moved content from Google Doc. \ud83d\udd04 Final review and sign-off on <code>project_plan.md</code>. M2 \u2b1c Rename <code>docs/index.md</code> to <code>project_plan.md</code>. M2 \u2b1c Update <code>nav</code> section in <code>mkdocs.yml</code>. M3 Point home to <code>project_plan.md</code>. \u2b1c Delete old <code>docs/milestones.md</code>. M6 \u2b1c Rename phase files (e.g., <code>phase-01-collection.md</code>) M4 Reflect new numbering &amp; report focus."},{"location":"project-status/#phase-1-data-collection","title":"Phase 1: Data Collection","text":"<p>(Objective: To gather all necessary raw data from defined sources - see <code>project_plan.md</code> for details)</p> <p>Major Milestones: - \u2b1c M1: All data sources and specific data fields fully identified and documented in <code>project_plan.md</code>. (Target: YYYY-MM-DD) - \u2b1c M2: Web scraping scripts for HKJC racecards developed and robustly tested. (Target: YYYY-MM-DD) - \u2b1c M3: Web scraping scripts for HKJC race results developed and robustly tested. (Target: YYYY-MM-DD) - \u2b1c M4: Historical data (target: X seasons/Y race days) successfully collected. (Target: YYYY-MM-DD) - \u2b1c M5: Initial storage solution for raw data implemented and populated. (Target: YYYY-MM-DD) - \u2b1c M6: Report: <code>docs/phase-01-collection.md</code> completed and reviewed. (Target: YYYY-MM-DD)</p> <p>Detailed Tasks &amp; Issues:</p> Status Task / Issue Description Milestone Notes / Resolution \u2b1c Draft structure for <code>docs/phase-01-collection.md</code> report. M6 \u2b1c Research and select Python library for web scraping. M2 Options: BeautifulSoup, Playwright, Scrapy. \u2b1c Implement <code>scrape_race_dates()</code> function. M2 \u2b1c Implement <code>scrape_racecard(race_date_url)</code> function. M2 \u2b1c Implement <code>scrape_results(race_date_url)</code> function. M3 \u2b1c Define error handling and retry logic for scrapers. M7 E.g., for network issues, unexpected page structure. \u2b1c Test scrapers on a diverse sample of 5-10 race days. M8 Include different tracks, number of races. \u2b1c Address Issue: Potential for IP blocking during scraping. You Monitor, implement delays, consider proxy/VPN if necessary. \u2b1c Define schema for raw racecard data (JSON/CSV). M9 \u2b1c Define schema for raw results data (JSON/CSV). M10"},{"location":"project-status/#phase-2-data-cleansing-preprocessing","title":"Phase 2: Data Cleansing &amp; Preprocessing","text":"<p>(Objective: To transform raw data into a clean, consistent, and usable format - see <code>project_plan.md</code> for details)</p> <p>Major Milestones: - \u2b1c M1: Data quality issues from raw data fully profiled and documented. (Target: YYYY-MM-DD) - \u2b1c M2: Comprehensive data cleansing rules and procedures defined. (Target: YYYY-MM-DD) - \u2b1c M3: Scripts for all data type conversions and value standardizations developed. (Target: YYYY-MM-DD) - \u2b1c M4: Strategy for handling missing values and outliers defined and implemented. (Target: YYYY-MM-DD) - \u2b1c M5: Cleansed dataset validated against defined quality criteria and schema. (Target: YYYY-MM-DD) - \u2b1c M6: Report: <code>docs/phase-02-cleansing.md</code> completed and reviewed. (Target: YYYY-MM-DD)</p> <p>Detailed Tasks &amp; Issues:</p> Status Task / Issue Description Milestone Notes / Resolution \u2b1c Draft structure for <code>docs/phase-02-cleansing.md</code> report. M6 \u2b1c Perform data profiling on raw collected data (from Phase 1). M1 Use Pandas Profiling or custom scripts. \u2b1c Develop script for converting date/time fields. M3 Ensure consistent <code>YYYY-MM-DD HH:MM:SS</code> format. \u2b1c Script to standardize categorical values (e.g., track conditions). M3 \u2b1c Research and select imputation techniques for <code>HORSE_WEIGHT</code>. M4 Mean, median, model-based? \u2b1c Implement chosen imputation for <code>HORSE_WEIGHT</code>. M4 \u2b1c Identify and handle outliers in <code>FINISH_TIME</code>. M4 E.g., Capping, removal based on IQR."},{"location":"project-status/#phase-3-exploratory-data-analysis-eda","title":"Phase 3: Exploratory Data Analysis (EDA)","text":"<p>(Objective: To understand data patterns, relationships, and formulate initial hypotheses - see <code>project_plan.md</code> for details)</p> <p>Major Milestones: - \u2b1c M1: Descriptive statistics generated and analyzed for all key variables. (Target: YYYY-MM-DD) - \u2b1c M2: Key visualizations (distributions, correlations, time series) created and interpreted. (Target: YYYY-MM-DD) - \u2b1c M3: Significant patterns, anomalies, and relationships documented. (Target: YYYY-MM-DD) - \u2b1c M4: Initial hypotheses about predictive factors formulated and listed. (Target: YYYY-MM-DD) - \u2b1c M5: Report: <code>docs/phase-03-eda.md</code> completed and reviewed. (Target: YYYY-MM-DD)</p> <p>Detailed Tasks &amp; Issues:</p> Status Task / Issue Description Milestone Notes / Resolution \u2b1c Draft structure for <code>docs/phase-03-eda.md</code> report. M5 \u2b1c Set up EDA Jupyter notebook (<code>notebooks/01-race-eda.ipynb</code>). M1 Load cleansed data. \u2b1c Generate histograms and density plots for numerical features. M2 E.g., <code>FINISH_TIME</code>, <code>STARTING_ODDS</code>. \u2b1c Create bar charts for categorical feature frequencies. M2 E.g., <code>COURSE</code>, <code>GOING</code>. \u2b1c Calculate and visualize correlation matrix. M2 Identify highly correlated features. \u2b1c Box plots for numerical features grouped by key categories. M2 E.g., <code>FINISH_TIME</code> by <code>CLASS</code>. \u2b1c Investigate any surprising findings from initial plots. M3"},{"location":"project-status/#phase-4-feature-engineering","title":"Phase 4: Feature Engineering","text":"<p>(Objective: To create new predictive features from the cleansed data - see <code>project_plan.md</code> for details)</p> <p>Major Milestones: - \u2b1c M1: List of potential engineered features brainstormed and prioritized. (Target: YYYY-MM-DD) - \u2b1c M2: Initial set of ~20-30 engineered features designed and documented. (Target: YYYY-MM-DD) - \u2b1c M3: Scripts to generate these engineered features implemented and tested. (Target: YYYY-MM-DD) - \u2b1c M4: Engineered feature set validated and stored. (Target: YYYY-MM-DD) - \u2b1c M5: Report: <code>docs/phase-04-features.md</code> completed and reviewed. (Target: YYYY-MM-DD)</p> <p>Detailed Tasks &amp; Issues:</p> Status Task / Issue Description Milestone Notes / Resolution \u2b1c Draft structure for <code>docs/phase-04-features.md</code> report. M5 \u2b1c Create rolling average features for horse past performance. M3 E.g., avg finish pos last 3/5 races. \u2b1c Calculate speed figures based on time and distance. M3 \u2b1c Encode categorical variables (e.g., one-hot, target encoding). M3 \u2b1c Create interaction features (e.g., jockey-trainer win rate). M3"},{"location":"project-status/#phase-5-model-development","title":"Phase 5: Model Development","text":"<p>(Objective: To train and select predictive models - see <code>project_plan.md</code> for details) (Structure: Major Milestones list, then Detailed Tasks &amp; Issues table) ... (Content to be filled similarly) ...</p>"},{"location":"project-status/#phase-6-model-evaluation","title":"Phase 6: Model Evaluation","text":"<p>(Objective: To rigorously assess model performance and potential profitability - see <code>project_plan.md</code> for details) (Structure: Major Milestones list, then Detailed Tasks &amp; Issues table) ... (Content to be filled similarly) ...</p>"},{"location":"project-status/#phase-7-deployment","title":"Phase 7: Deployment","text":"<p>(Objective: To set up a system for generating predictions on new races - see <code>project_plan.md</code> for details) (Structure: Major Milestones list, then Detailed Tasks &amp; Issues table) ... (Content to be filled similarly) ...</p>"},{"location":"project-status/#phase-8-ongoing-management-iteration","title":"Phase 8: Ongoing Management &amp; Iteration","text":"<p>(Objective: To maintain the system, monitor performance, and plan for future improvements - see <code>project_plan.md</code> for details) (Structure: Major Milestones list, then Detailed Tasks &amp; Issues table) ... (Content to be filled similarly) ...</p> <p>---</p>"},{"location":"project_plan/","title":"Hong Kong Racing Project: Master Plan","text":"<p>This document outlines the technical plan for building a handicapping and wagering system for thoroughbred horse racing in Hong Kong, with the primary objective of achieving sustainable financial gain through data-driven strategies. It serves as the central reference for project scope, methodology, and planned execution across all phases.</p>"},{"location":"project_plan/#1-project-definition-and-establishment","title":"1. Project Definition and Establishment","text":""},{"location":"project_plan/#11-introduction-and-aims","title":"1.1. Introduction and Aims","text":""},{"location":"project_plan/#111-project-objective","title":"1.1.1. Project Objective","text":"<ul> <li>Goal: Develop and implement a machine learning-based handicapping and wagering system aimed at generating a positive return on investment (ROI) from Hong Kong horse racing.</li> <li>Team: Solo project with assistance from AI (e.g., Google Gemini).</li> <li>Broader Context: This project focuses on the technical development of a predictive system for financial gain within the domain of horse race wagering. It's acknowledged that this operates within the context of gambling, and responsible practices should guide any potential future application of the system's outputs.</li> </ul>"},{"location":"project_plan/#112-document-purpose","title":"1.1.2. Document Purpose","text":"<p>This <code>project_plan.md</code> (and the accompanying MkDocs site) serves several key purposes: * Centralized Record: Acts as the primary repository for documenting all project plans, methodologies, data sources, and strategic decisions made throughout the project lifecycle. * Shared Context for AI Collaboration: Functions as a persistent memory and context for ongoing work and discussions with AI assistants. Referencing and updating this plan and related phase reports ensures continuity. * Single Source of Truth: Establishes a definitive reference point for the project's scope, workflow, tools, and challenges. * Tracking Evolution: While this document outlines the plan, individual phase reports (<code>docs/phase-XX-name.md</code>) will track the execution and evolution of the project.</p>"},{"location":"project_plan/#12-resource-and-environment-configuration","title":"1.2. Resource and Environment Configuration","text":""},{"location":"project_plan/#121-hardware","title":"1.2.1. Hardware","text":"<ul> <li>Local machine: MacBook Air, 8GB RAM, Apple Silicon (M1 chip).</li> <li>Online: Standard Colab CPU Environment (e.g., Intel Xeon, ~13 GB RAM). Google Colab free tier also provides variable access to GPU and TPU accelerators.</li> </ul>"},{"location":"project_plan/#122-software-and-cloud-services-primarily-google-ecosystem","title":"1.2.2. Software and Cloud Services (Primarily Google Ecosystem)","text":"<ul> <li>Google BigQuery: Central data warehouse for storing historical and current race data. Chosen for scalability, Colab integration, and cost-effectiveness within its free tier.</li> <li>Google Colab: Primary environment for data scraping, cleaning, analysis, feature engineering, visualization, and ML model development.</li> <li>Google Sheets: Used as an intermediary data hub, for manual input/correction if necessary, and potentially for controlling updates via Apps Script (original plan).</li> <li>Google Apps Script: Planned for automating data update processes (e.g., Sheets to BigQuery).</li> <li>Google Cloud Storage (GCS): Considered for intermediary data storage (e.g., Parquet files) to optimize data transfer between BigQuery and Colab.</li> <li>Python Libraries:<ul> <li>Data Acquisition: Beautiful Soup (or potentially Playwright/Selenium if dynamic content requires it).</li> <li>Data Manipulation &amp; Analysis: pandas, NumPy, Polars.</li> <li>Cloud Interaction: <code>google-cloud-bigquery</code>, <code>gspread</code>.</li> <li>Visualization: Plotly, Matplotlib, Seaborn.</li> <li>Machine Learning: Scikit-learn, TensorFlow, PyTorch, XGBoost, LightGBM (specific choices in modeling phase).</li> </ul> </li> <li>Version Control: Git, managed via a GitHub repository.</li> <li>Documentation: MkDocs with the Material theme.</li> </ul>"},{"location":"project_plan/#123-development-environment","title":"1.2.3. Development Environment","text":"<ul> <li>Google Colab: Primary for computationally intensive tasks (data processing, ML).</li> <li>Visual Studio Code (VS Code) - Local: For developing helper scripts, managing the project repository, and MkDocs site generation/preview.</li> </ul>"},{"location":"project_plan/#13-proposed-workflow-outline-original-hybrid-concept","title":"1.3. Proposed Workflow Outline (Original Hybrid Concept)","text":""},{"location":"project_plan/#131-rationale-for-hybrid-workflow-sheets-bigquery-colab","title":"1.3.1. Rationale for Hybrid Workflow (Sheets -&gt; BigQuery -&gt; Colab)","text":"<p>(This section reflects the original thinking from the Google Doc. The actual implementation might evolve and will be documented in phase reports.) The initially chosen workflow utilized a hybrid approach: Google Sheets (for easily editable data), BigQuery (External Tables linking to Sheets, then materializing into Native Tables for performance), and Google Colab for analysis and ML. * Ease of Data Correction: Sheets as the primary editable source. * Analytical Performance: Native BigQuery tables for querying. * Scalability: BigQuery's inherent scalability. * Resource Efficiency: Colab for computation. * Integration &amp; Control: Google ecosystem synergy. * Cost-Effectiveness: Leveraging free tiers.</p>"},{"location":"project_plan/#132-alternative-methodologies-considered-original-assessment","title":"1.3.2. Alternative Methodologies Considered (Original Assessment)","text":"<ul> <li>DuckDB (Local) + Colab: Rejected due to local RAM limits and less straightforward data correction than Sheets.</li> <li>Pandas/Polars (Local only): Rejected due to RAM limits and lack of persistence/scalability.</li> <li>SQLite (Local): Rejected due to performance for analytical queries and local resource limits.</li> <li>PostgreSQL (Local): Rejected due to overhead on local hardware.</li> <li>Snowflake (Cloud): Rejected due to lack of a comparable free tier to BigQuery.</li> </ul>"},{"location":"project_plan/#phase-1-data-collection-and-storage","title":"Phase 1: Data Collection and Storage","text":"<p>(This phase focuses on acquiring all necessary raw data and establishing initial storage.)</p>"},{"location":"project_plan/#11-data-sources","title":"1.1. Data Sources","text":"<ul> <li>Primary Source: Hong Kong Jockey Club (HKJC) official website.</li> <li>Data Types: Racecards (upcoming races), race results (historical), horse details, trackwork records, stewards' reports, etc.</li> <li>Historical Data Consideration: An initial dataset was acquired from a third-party vendor (pre-December 2023) with known minor inconsistencies. Data from December 2023 onwards is to be scraped directly. The project will prioritize direct scraping for ongoing data integrity.</li> </ul>"},{"location":"project_plan/#12-data-acquisition-methodology","title":"1.2. Data Acquisition Methodology","text":"<ul> <li>Primary Method: Web scraping using Python libraries (e.g., Beautiful Soup, potentially Playwright/Selenium if needed for dynamic content) executed in Google Colab or local scripts.</li> <li>API Limitations: HKJC does not offer a public API for comprehensive race data, necessitating web scraping.</li> <li>Scope per Session: Scraping will likely be batched (e.g., per race day) to manage load and avoid detection.</li> <li>Maintenance: Scraping scripts will require ongoing maintenance due to potential HKJC website changes.</li> </ul>"},{"location":"project_plan/#13-preliminary-data-formatting-post-scraping","title":"1.3. Preliminary Data Formatting (Post-Scraping)","text":"<ul> <li>Environment: Google Colab or local Python scripts.</li> <li>Objective: Ensure consistency, accuracy, and control over data format before storage. Includes data type enforcement, string manipulation, and basic structural validation.</li> <li>Rationale: Python scripts provide reproducibility and can handle complex logic more robustly than manual formatting or simple spreadsheet functions.</li> </ul>"},{"location":"project_plan/#14-data-storage-strategy","title":"1.4. Data Storage Strategy","text":"<ul> <li>Raw Data Storage: Initially, scraped data might be stored in flat files (JSON, CSV) in Google Cloud Storage or locally.</li> <li>Structured Data Storage: Google BigQuery will serve as the primary data warehouse for cleansed and structured data.<ul> <li>Schema Definition: A detailed data dictionary (see Appendix A) will define table structures and field types in BigQuery.</li> </ul> </li> <li>Intermediary Storage (Original Plan): Google Sheets was considered as an editable central repository before loading to BigQuery. This may be revised for a more direct GCS/local files to BigQuery pipeline.</li> <li>Partitioning: For BigQuery tables, partitioning (e.g., by race date/year) will be considered if data volume grows significantly, to optimize query performance and costs. Not planned initially given projected data sizes.</li> <li>Update Frequency: New race data will be collected and processed typically twice weekly, aligned with the HKJC racing calendar. The process will be automated as much as possible.</li> </ul>"},{"location":"project_plan/#phase-2-data-cleansing-and-preprocessing","title":"Phase 2: Data Cleansing and Preprocessing","text":"<p>(This phase focuses on transforming raw collected data into a clean, consistent, and usable dataset.)</p>"},{"location":"project_plan/#21-overview-of-collected-data","title":"2.1. Overview of Collected Data","text":"<p>The dataset will encompass: * Race identification and context (Date, Course, Race Number, Class, Distance, Going, Prize Money, etc.). * Horse performance metrics (Finishing Place, Finish Time, Sectional Times, Weight Carried, Draw, etc.). * Betting information (Win/Place Odds). * Jockey and Trainer details. * Pre-race horse information (Horse Weight, Rating, Rest Days, Gear, etc.). * (Refer to Appendix A: Data Dictionary for detailed schema).</p>"},{"location":"project_plan/#22-cleansing-procedure","title":"2.2. Cleansing Procedure","text":"<ul> <li>Environment: Primarily Google Colab or local Python scripts, with results stored in BigQuery.</li> <li>Key Steps:<ul> <li>Data Type Enforcement: Ensure columns match predefined types (integer, float, string, date).</li> <li>String Manipulation: Trim whitespace, standardize capitalization, handle special characters.</li> <li>Categorical Value Standardization: Ensure consistency in fields like <code>COURSE</code>, <code>CLASS</code>, <code>GOING</code>. Map variations to standard values.</li> <li>Handling Specific Non-Numeric/Placeholder Strings: Address values like 'UNRATED', 'GRIFFIN', 'DEBUT' in fields that are otherwise numeric or categorical.</li> <li>Addressing Known Inconsistencies: Programmatic fixes for known issues in historical data.</li> </ul> </li> </ul>"},{"location":"project_plan/#23-data-validation-and-format-consistency","title":"2.3. Data Validation and Format Consistency","text":"<ul> <li>Objective: Rigorously validate data against the schema in Appendix A.</li> <li>Checks: Data type verification, value range checks, categorical value consistency, basic relational integrity checks (e.g., consistent IDs).</li> <li>Discrepancy Handling: Refine cleansing scripts or (as a last resort for historical data) document manual corrections.</li> </ul>"},{"location":"project_plan/#24-management-of-missing-valuesoutliers","title":"2.4. Management of Missing Values/Outliers","text":"<ul> <li>Approach: Strategies will be determined based on EDA findings (Phase 3) and model requirements.</li> <li>Missing Values: Options include deletion (rows/columns), mean/median/mode imputation, regression imputation, or model-based imputation.</li> <li>Outliers: Identification (e.g., IQR, Z-scores) followed by potential capping, transformation, or removal.</li> </ul>"},{"location":"project_plan/#25-data-transformation-preparation-for-modeling","title":"2.5. Data Transformation (Preparation for Modeling)","text":"<ul> <li>Encoding Categorical Variables: Convert string categories (e.g., <code>CLASS</code>, <code>GOING</code>) into numerical representations (One-Hot, Label, Target Encoding).</li> <li>Numerical Scaling: Apply standardization or normalization if required by specific models.</li> <li>Other Transformations: Log transforms, polynomial features, etc., based on EDA and model needs.</li> </ul>"},{"location":"project_plan/#phase-3-exploratory-data-analysis-eda","title":"Phase 3: Exploratory Data Analysis (EDA)","text":"<p>(This phase focuses on understanding the data through querying, statistics, and visualizations to uncover patterns and formulate hypotheses.)</p>"},{"location":"project_plan/#31-data-querying","title":"3.1. Data Querying","text":"<ul> <li>Source: Cleansed data tables in Google BigQuery.</li> <li>Environment: Google Colab using the <code>google-cloud-bigquery</code> Python library.</li> <li>Process: Construct SQL queries in Colab, execute against BigQuery, load results into Pandas/Polars DataFrames for analysis.</li> </ul>"},{"location":"project_plan/#32-descriptive-statistics","title":"3.2. Descriptive Statistics","text":"<ul> <li>Tools: Python libraries (Pandas, NumPy) in Colab.</li> <li>Numerical Variables: Calculate count, mean, median, std dev, min, max, percentiles, skewness, kurtosis.</li> <li>Categorical Variables: Frequency counts, unique values, mode.</li> </ul>"},{"location":"project_plan/#33-visualization","title":"3.3. Visualization","text":"<ul> <li>Primary Tool: Plotly for interactive plots in Colab. Seaborn and Matplotlib as alternatives.</li> <li>Univariate Analysis: Histograms, density plots, box plots, bar charts.</li> <li>Bivariate/Multivariate Analysis: Scatter plots, correlation heatmaps, grouped plots, time series plots (if applicable).</li> </ul>"},{"location":"project_plan/#34-initial-observations-and-hypothesis-formulation","title":"3.4. Initial Observations and Hypothesis Formulation","text":"<ul> <li>Synthesize findings from statistics and visualizations.</li> <li>Identify significant patterns, trends, anomalies, and correlations.</li> <li>Formulate initial hypotheses about factors influencing race outcomes.</li> <li>Identify areas needing further investigation. These insights will guide Feature Engineering (Phase 4).</li> </ul>"},{"location":"project_plan/#phase-4-feature-engineering","title":"Phase 4: Feature Engineering","text":"<p>(This phase focuses on creating new, potentially more predictive variables from the cleaned dataset.)</p>"},{"location":"project_plan/#41-methodology","title":"4.1. Methodology","text":"<ul> <li>Environment: Google Colab using Pandas, NumPy, Polars, and Scikit-learn.</li> <li>Approach: Iterative process based on domain knowledge, EDA insights, and preliminary model testing.</li> <li>Goal: Transform base data into a richer feature set that captures complex racing dynamics.</li> </ul>"},{"location":"project_plan/#42-initial-feature-set","title":"4.2. Initial Feature Set","text":"<ul> <li>The starting point is the cleansed, validated dataset (approx. 40-50 core variables as per Appendix A).</li> </ul>"},{"location":"project_plan/#43-target-feature-set","title":"4.3. Target Feature Set","text":"<ul> <li>Strategically expand the initial set. The focus is on developing hypothesized predictive features and validating their impact, rather than a fixed target number of features.</li> </ul>"},{"location":"project_plan/#44-example-categories-of-engineered-features","title":"4.4. Example Categories of Engineered Features","text":"<ul> <li>Form &amp; Consistency: Rolling win/place percentages, average finishing position (recent races), days since last win.</li> <li>Speed &amp; Pace: Calculated speed figures, sectional speed ratings, comparisons to class/distance/track averages.</li> <li>Jockey/Trainer Statistics: Win/place rates (overall, by course, distance, class, horse combination), recent form.</li> <li>Odds-Based Features: Odds movement (if available), probability derived from odds, value indicators (odds vs. finish).</li> <li>Class &amp; Weight Related: Rating relative to class, weight carried relative to past or standard weights.</li> <li>Interaction &amp; Derived Features: Jockey win rate on this course, horse average speed at this distance.</li> <li>Lagged Variables: Performance metrics from the previous race.</li> </ul>"},{"location":"project_plan/#45-feature-engineering-approach","title":"4.5. Feature Engineering Approach","text":"<ul> <li>Iterative: Develop, test (correlation, feature importance from simple models, backtesting), refine/discard.</li> <li>New ideas may emerge during modeling and evaluation.</li> </ul>"},{"location":"project_plan/#46-roles-of-bigquery-and-colab","title":"4.6. Roles of BigQuery and Colab","text":"<ul> <li>BigQuery: Source for cleaned base data. Potentially store validated engineered features in new tables/views for efficient reuse.</li> <li>Colab: Primary environment for implementing complex feature calculation logic.</li> </ul>"},{"location":"project_plan/#phase-5-model-development-and-training","title":"Phase 5: Model Development and Training","text":"<p>(This phase involves selecting, training, and optimizing machine learning models to predict race outcomes.)</p>"},{"location":"project_plan/#51-model-selection","title":"5.1. Model Selection","text":"<ul> <li>Target Variable(s): To be clearly defined (e.g., predict win probability, predict top N finish, predict expected ROI).</li> <li>Baseline Models: Start with simpler, interpretable models (Logistic Regression, Linear Regression, Decision Trees, Random Forests).</li> <li>Advanced Models: Explore Gradient Boosting Machines (XGBoost, LightGBM, CatBoost), Neural Networks (MLPs, potentially RNNs/LSTMs if sequential data like sectional times are heavily used).</li> <li>Selection Criteria: Predictive performance (via backtesting), interpretability, computational cost, training time.</li> </ul>"},{"location":"project_plan/#52-training-environment","title":"5.2. Training Environment","text":"<ul> <li>Primary: Google Colab, leveraging free-tier GPU/TPU accelerators for computationally intensive models.</li> </ul>"},{"location":"project_plan/#53-validation-approach","title":"5.3. Validation Approach","text":"<ul> <li>Primary Method: Rigorous Backtesting.<ul> <li>Simulate training on historical data up to a point and evaluating on subsequent unseen historical data.</li> <li>Employ time-series aware validation (e.g., walk-forward validation) to prevent data leakage.</li> </ul> </li> </ul>"},{"location":"project_plan/#54-hyperparameter-optimization","title":"5.4. Hyperparameter Optimization","text":"<ul> <li>For promising models, optimize hyperparameters to maximize performance on validation sets.</li> <li>Techniques: Grid Search, Randomized Search, Bayesian Optimization (using libraries like Optuna or Scikit-learn tools).</li> </ul>"},{"location":"project_plan/#phase-6-model-evaluation-and-validation","title":"Phase 6: Model Evaluation and Validation","text":"<p>(This phase focuses on rigorously evaluating model performance, especially in the context of wagering profitability.)</p>"},{"location":"project_plan/#61-performance-metrics","title":"6.1. Performance Metrics","text":"<ul> <li>Primary Focus (Wagering Profitability):<ul> <li>Return on Investment (ROI).</li> <li>Hit Rate (Win Prediction Accuracy, Place Prediction Accuracy).</li> </ul> </li> <li>Standard ML Metrics (Task-Dependent):<ul> <li>Classification: Accuracy, Precision, Recall, F1-score, Log Loss, AUC-ROC, AUC-PR.</li> <li>Regression (if predicting rank/time): MAE, RMSE.</li> </ul> </li> </ul>"},{"location":"project_plan/#62-backtesting-outcomes","title":"6.2. Backtesting Outcomes","text":"<ul> <li>Present detailed quantitative results from backtesting simulations.</li> <li>Compare different models and hyperparameter configurations across historical validation periods.</li> </ul>"},{"location":"project_plan/#63-wagering-strategy-simulation","title":"6.3. Wagering Strategy Simulation","text":"<ul> <li>Simulate model use within defined wagering strategies during backtesting.</li> <li>Examples: Fixed Stakes, Percentage Stakes (e.g., fixed percentage of bankroll), Kelly Criterion (adjusting bet size based on perceived edge and odds).</li> <li>Simulation must account for historical odds and race results.</li> </ul>"},{"location":"project_plan/#64-profitability-analysis","title":"6.4. Profitability Analysis","text":"<ul> <li>In-depth analysis of simulated financial performance (total profit/loss, ROI, drawdown, risk-adjusted returns).</li> <li>Assess feasibility of achieving the project's primary objective (positive ROI).</li> </ul>"},{"location":"project_plan/#65-iterative-refinement","title":"6.5. Iterative Refinement","text":"<ul> <li>Evaluation results drive improvements:<ul> <li>Revisit Feature Engineering (Phase 4).</li> <li>Revisit Model Selection (Phase 5.1).</li> <li>Revisit Hyperparameter Optimization (Phase 5.4).</li> <li>Analyze errors to understand model strengths/weaknesses.</li> </ul> </li> </ul>"},{"location":"project_plan/#phase-7-deployment-and-monitoring-future-phase","title":"Phase 7: Deployment and Monitoring (Future Phase)","text":"<p>(This phase outlines operationalizing the model and monitoring its ongoing performance.)</p>"},{"location":"project_plan/#71-prediction-generation-workflow","title":"7.1. Prediction Generation Workflow","text":"<ul> <li>End-to-end process for generating predictions/insights for upcoming race days:<ul> <li>Acquire new racecard data.</li> <li>Apply preprocessing and feature engineering steps consistently.</li> <li>Load trained model.</li> <li>Generate predictions.</li> <li>Store/present predictions for decision-making.</li> </ul> </li> <li>Define automation level and tools.</li> </ul>"},{"location":"project_plan/#72-performance-monitoring","title":"7.2. Performance Monitoring","text":"<ul> <li>Track real-world effectiveness and profitability:<ul> <li>Collect actual race results.</li> <li>Compare outcomes against predictions.</li> <li>Track ROI based on simulated/actual wagers.</li> <li>Monitor for model drift or changes in data distributions.</li> </ul> </li> <li>Define metrics, frequency, and tools.</li> </ul>"},{"location":"project_plan/#73-retraining-approach","title":"7.3. Retraining Approach","text":"<ul> <li>Strategy for periodic model retraining:<ul> <li>Triggers: Performance degradation, fixed schedule, or significant new data accumulation.</li> <li>Process: Reuse pipelines from Phases 2-6 with updated data.</li> <li>Validation: Retrained models validated via backtesting before deployment.</li> </ul> </li> </ul>"},{"location":"project_plan/#phase-8-project-management-and-continuous-improvement","title":"Phase 8: Project Management and Continuous Improvement","text":"<p>(This section covers ongoing project aspects, learnings, and future considerations.)</p>"},{"location":"project_plan/#81-potential-challenges-and-resolutions-ongoing-log","title":"8.1. Potential Challenges and Resolutions (Ongoing Log)","text":"<p>(This list will be dynamic and updated in phase reports or a dedicated decisions/risks log as they arise.) * Cloud Costs (BigQuery/GCP): Monitor usage, optimize queries, leverage free tiers. * Feature Engineering Complexity: Iterative approach, start simple, leverage domain knowledge and EDA. * Model Validation &amp; Profitability: Rigorous backtesting, realistic wagering simulation. * Data Pipeline Robustness: Error handling, logging, monitoring. * Data Integrity &amp; Correction: Ongoing validation checks, clear process for corrections. * Web Scraping Maintenance: Monitor for HKJC site changes, allocate time for script updates.</p>"},{"location":"project_plan/#82-data-size-projections-and-growth","title":"8.2. Data Size Projections and Growth","text":"<ul> <li>Initial dataset (18 years, ~171k rows, 40-50 cols): ~20-50 MB.</li> <li>With engineered features (~150-200 cols): ~50-100 MB.</li> <li>Long-term growth (next 15-20 years): Potentially doubling to ~200-400 MB.</li> <li>These volumes are manageable within BigQuery free tiers and Colab.</li> </ul>"},{"location":"project_plan/#83-tools-summary","title":"8.3. Tools Summary","text":"<ul> <li>Data Storage &amp; Querying: Google BigQuery.</li> <li>Data Processing &amp; Modeling: Python in Google Colab (Pandas, Scikit-learn, etc.).</li> <li>Web Scraping: Python (Beautiful Soup / Playwright).</li> <li>Version Control: Git / GitHub.</li> <li>Documentation: MkDocs (Material theme).</li> <li>Project Management / Task Tracking: <code>docs/project-status.md</code>.</li> <li>(Original) Intermediary Data/Control: Google Sheets, Google Apps Script (may be revised).</li> </ul>"},{"location":"project_plan/#84-future-considerations","title":"8.4. Future Considerations","text":"<ul> <li>Advanced Deployment: If successful, explore more sophisticated deployment (e.g., dedicated prediction server, API).</li> <li>Alternative Data Sources: If HKJC scraping becomes untenable (unlikely to be better external sources).</li> <li>Deeper Model Exploration: More complex architectures if justified by performance.</li> </ul>"},{"location":"project_plan/#85-version-control-strategy","title":"8.5. Version Control Strategy","text":"<ul> <li>All project code, documentation, and configuration files will be version controlled using Git, hosted on a GitHub repository.</li> <li>Branches will be used for feature development and experimentation (e.g., <code>feature/new-scraper</code>, <code>experiment/new-model-arch</code>).</li> <li>The <code>main</code> branch will represent the stable, working version of the project.</li> <li>Commits should be descriptive and atomic.</li> </ul>"},{"location":"project_plan/#86-references","title":"8.6. References","text":"<p>(Placeholder for links to key documentation, research papers, articles, etc., consulted during the project.) * HKJC Website: <code>[Link to be added]</code> * Pandas Documentation: <code>https://pandas.pydata.org/pandas-docs/stable/</code> * Scikit-learn Documentation: <code>https://scikit-learn.org/stable/</code> * MkDocs Material Theme: <code>https://squidfunk.github.io/mkdocs-material/</code></p>"},{"location":"project_plan/#appendix-a-data-dictionary","title":"Appendix A: Data Dictionary","text":"<p>(This defines the planned schema for data stored in BigQuery. It may evolve.)</p>"},{"location":"project_plan/#a1-table-race_details","title":"A.1. Table: <code>race_details</code>","text":"<p>(Contains details specific to each race event)</p> Column Name Data Type Description Notes/Example <code>RACE_ID</code> INTEGER Unique identifier for each race. Format: <code>YYYYMMDD</code> + <code>RACE_NUM_SEASON</code> (e.g., <code>20240414581</code>). Seasonal race number is 3 digits (e.g., <code>581</code>). <code>DATE</code> DATE Date the race was held. Format: <code>YYYY-MM-DD</code> <code>COURSE</code> STRING Racecourse where the race took place. Values: <code>Sha Tin</code>, <code>Sha Tin (AWT)</code>, <code>Happy Valley</code> <code>RACE_NUM_SEASON</code> INTEGER The sequential number of the race within the racing season. e.g., <code>581</code> <code>CLASS</code> STRING The class of the race. e.g., <code>Class 1</code>, <code>Group 1</code>, <code>Griffin</code> <code>DISTANCE</code> INTEGER The race distance in meters. e.g., <code>1200</code>, <code>1650</code> <code>RANKING</code> STRING The rating bracket for the race. e.g., <code>85-60</code> <code>GOING</code> STRING Description of surface going. e.g., <code>GOOD</code>, <code>YIELDING (WET SLOW)</code> <code>RACE_DESCRIPTION</code> STRING The official title or name of the race. e.g., <code>THE HONG KONG EXCHANGES CHALLENGE CUP HANDICAP</code> <code>TRACK_CONFIG</code> STRING The track configuration for the race. e.g., <code>TURF - \"C\" Course</code>, <code>ALL WEATHER TRACK</code> <code>PRIZE</code> INTEGER The total prize money in HKD for the race. <code>PEN_READING</code> FLOAT Penetrometer reading (turf) or Clegg hammer reading (all-weather track)."},{"location":"project_plan/#a2-table-race_card-or-runners","title":"A.2. Table: <code>race_card</code> (or <code>runners</code>)","text":"<p>(Contains details for each horse participating in a race, pre-race information)</p> Column Name Data Type Description Notes/Example <code>RUNNER_ID</code> STRING Unique identifier for a horse in a specific race. Format: <code>RACE_ID</code> + <code>*</code> + <code>HORSE_ID</code> (e.g., <code>20240414581*CLEARWIN*H255</code>) <code>RACE_ID</code> INTEGER Foreign key referencing <code>race_details.RACE_ID</code>. <code>HORSE_ID</code> STRING Unique identifier for the horse (stable across races). Format: <code>HORSE_NAME</code> (no spaces/special chars) + <code>*</code> + <code>HORSE_CODE</code> (e.g., <code>CLEARWIN*H255</code>) <code>HORSE_NUM</code> INTEGER The number assigned to the horse for that race (usually printed on saddle cloth). e.g., <code>1</code>, <code>2</code>, ... <code>14</code> <code>WEIGHT</code> INTEGER Total weight carried by the horse (lbs), including jockey and gear. <code>HORSE_WEIGHT</code> INTEGER Declared weight of the horse (lbs), usually taken a day or two before the race. <code>DRAW</code> INTEGER Starting gate (barrier) number. Lower numbers are closer to the inside rail. <code>JOCKEY</code> STRING Jockey's name. <code>TRAINER</code> STRING Trainer's name. <code>RATING</code> STRING Official HKJC rating of the horse at the time of the race. Numeric string, or <code>UNRATED</code> (overseas), <code>GRIFFIN</code> (novices). <code>REST_DAYS</code> STRING Number of days since the horse's last run. Numeric string, or <code>DEBUT</code> (first run in HK or ever). <code>RACE_AGE</code> STRING Age of the horse at the time of the race. Numeric string, or <code>UNKNOWN</code>. <code>GEAR</code> STRING Symbols representing gear used by the horse. e.g., <code>PC/XB/TT</code>. See Gear Key below. <p>Gear Key (Example - to be confirmed from HKJC source): * <code>B</code>: Blinkers * <code>CP</code>: Sheepskin Cheek Pieces * <code>TT</code>: Tongue Tie * <code>XB</code>: Crossed Nose Band * <code>P</code>: Pacifier * <code>V</code>: Visor * <code>H</code>: Hood * <code>E</code>: Ear Plugs * <code>1</code> (suffix): First time using this gear.</p>"},{"location":"project_plan/#a3-table-race_results","title":"A.3. Table: <code>race_results</code>","text":"<p>(Contains the official results for each horse in a race)</p> Column Name Data Type Description Notes/Example <code>RUNNER_ID</code> STRING Foreign key referencing <code>race_card.RUNNER_ID</code>. <code>RACE_ID</code> INTEGER Foreign key referencing <code>race_details.RACE_ID</code>. <code>HORSE_ID</code> STRING Foreign key referencing horse identity. <code>FINISH_POS</code> INTEGER Final official finishing position. <code>1</code> for winner, <code>2</code> for second, etc. May include codes for non-finishers. <code>STARTING_ODDS</code> FLOAT Decimal odds of the horse just before race start. e.g., <code>1.8</code>, <code>10.5</code> <code>PLACE_PAYOUTS</code> FLOAT Decimal odds payout for a successful place bet. <code>0</code> or <code>NULL</code> if unplaced or no payout. <code>FINISH_TIME</code> FLOAT Finishing time of the horse in seconds. e.g., <code>71.52</code> <code>SEC_TIME_1</code> FLOAT Time taken to finish section 1 (seconds). <code>SEC_TIME_2</code> FLOAT Time taken to finish section 2 (seconds). <code>SEC_TIME_3</code> FLOAT Time taken to finish section 3 (seconds). <code>SEC_TIME_4</code> FLOAT Time taken to finish section 4 (seconds). (Sectional times depend on race distance and course) <code>SEC_TIME_5</code> FLOAT Time taken to finish section 5 (seconds). <code>SEC_TIME_6</code> FLOAT Time taken to finish section 6 (seconds). <code>DIST_BEATEN</code> FLOAT Distance beaten by the winner, in lengths (approx). <code>0</code> for winner. <code>INCIDENTS</code> STRING Notes on any racing incidents involving the horse during the race. e.g., \"Bumped at start\", \"Checked near 800m\""},{"location":"project_plan/#a4-table-horse_register","title":"A.4. Table: <code>horse_register</code>","text":"<p>(Contains static details for all registered horses)</p> Column Name Data Type Description Notes/Example <code>HORSE_ID</code> STRING Unique identifier for the horse. Format: <code>HORSE_NAME</code> (no spaces/specials) + <code>*</code> + <code>HORSE_CODE</code> <code>HORSE_CODE</code> STRING Unique code assigned by HKJC to differentiate horses (esp. same names). e.g., <code>H255</code>, <code>D466</code> <code>HORSE_NAME</code> STRING Official name of the horse. Raw name, e.g., \"DEAN'S ANGEL\" <code>SEX</code> STRING Horse's sex. <code>Gelding</code>, <code>Mare</code>, <code>Colt</code>, <code>Filly</code>, <code>Horse</code>, <code>Rig</code> <code>COLOR</code> STRING Color of the horse. e.g., <code>Bay</code>, <code>Chestnut</code> <code>COUNTRY_OF_ORIGIN</code> STRING Country where the horse was born. e.g., <code>AUS</code>, <code>NZ</code>, <code>IRE</code> <code>IMPORT_TYPE</code> STRING Category of entry into Hong Kong racing. <code>PP</code> (Privately Purchased), <code>PPG</code> (Privately Purchased Griffin), <code>ISG</code> (International Sale Griffin), <code>VIS</code> (Visitor) <code>SIRE</code> STRING The horse's father (sire). <code>DAM</code> STRING The horse's mother (dam). <code>DAM_SIRE</code> STRING The sire of the horse's dam (maternal grandsire). <code>OWNER</code> STRING Name(s) of the horse's owner(s). <p>Import Type Key (Example): * <code>PP</code>: Privately Purchased Horses (previously raced elsewhere). * <code>PPG</code>: Privately Purchased Griffins (unraced young horses). * <code>ISG</code>: International Sale Griffins (unraced horses from approved sales). * <code>VIS</code>: Visiting Invitational Horses (invited for specific major races).</p>"}]}